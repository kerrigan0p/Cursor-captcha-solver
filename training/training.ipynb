{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field image batch shape: torch.Size([8, 3, 210, 210])\n",
      "Draw image batch shape: torch.Size([8, 3, 50, 50])\n",
      "Label batch shape: torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "class CaptchaDetectionDataset(Dataset):\n",
    "    def __init__(self, field_folder, draw_folder, label_csv, \n",
    "                 transform_field=None, transform_draw=None, transform_label=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            field_folder (str): Path to the folder containing field images (210x210).\n",
    "            draw_folder (str): Path to the folder containing draw images (50x50).\n",
    "            label_csv (str): Path to the CSV file containing labels with columns:\n",
    "                             \"img_name\", \"x\", \"y\".\n",
    "            transform_field (callable, optional): Transform to apply to field images.\n",
    "            transform_draw (callable, optional): Transform to apply to draw images.\n",
    "            transform_label (callable, optional): Transform to apply to labels.\n",
    "        \"\"\"\n",
    "        self.field_folder = field_folder\n",
    "        self.draw_folder = draw_folder\n",
    "        self.labels_df = pd.read_csv(label_csv)\n",
    "        \n",
    "        # Optional transforms for images/labels\n",
    "        self.transform_field = transform_field\n",
    "        self.transform_draw = transform_draw\n",
    "        self.transform_label = transform_label\n",
    "        \n",
    "        # Create a list of image names from the CSV\n",
    "        self.img_names = self.labels_df[\"img_name\"].tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image file name from CSV\n",
    "        img_name = self.img_names[idx]\n",
    "        \n",
    "        # Construct full paths for field and draw images\n",
    "        field_img_path = os.path.join(self.field_folder, img_name)\n",
    "        draw_img_path = os.path.join(self.draw_folder, img_name)\n",
    "        \n",
    "        # Load images using PIL and convert them to RGB\n",
    "        field_img = Image.open(field_img_path).convert('RGB')\n",
    "        draw_img = Image.open(draw_img_path).convert('RGB')\n",
    "        \n",
    "        # Apply transformations if provided (for example, resizing, normalization, converting to tensor)\n",
    "        if self.transform_field:\n",
    "            field_img = self.transform_field(field_img)\n",
    "        if self.transform_draw:\n",
    "            draw_img = self.transform_draw(draw_img)\n",
    "        \n",
    "        # Extract label (x, y) from the CSV row and convert to a float tensor\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        x = float(row[\"x\"])\n",
    "        y = float(row[\"y\"])\n",
    "        label = torch.tensor([x, y], dtype=torch.float32)\n",
    "        if self.transform_label:\n",
    "            label = self.transform_label(label)\n",
    "        \n",
    "        return field_img, draw_img, label\n",
    "\n",
    "# Example usage:\n",
    "def create_train_dataset():\n",
    "    # Define transforms: Convert images to tensors, resize if needed, etc.\n",
    "    transform_field = transforms.Compose([\n",
    "        transforms.Resize((210, 210)),\n",
    "        transforms.ToTensor()  # Converts to tensor and scales pixel values to [0, 1]\n",
    "    ])\n",
    "    \n",
    "    transform_draw = transforms.Compose([\n",
    "        transforms.Resize((50, 50)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_folder = \"train_set/\"\n",
    "    # Folder paths\n",
    "    field_folder = train_folder + \"field\"   # e.g., images of size 210x210\n",
    "    draw_folder = train_folder + \"rotated_draw1\"       # e.g., images of size 50x50\n",
    "    label_csv = train_folder + \"rotated_draw1/augmented_labels.csv\"          # CSV with columns: \"img_name\", \"x\", \"y\"\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset1 = CaptchaDetectionDataset(field_folder, draw_folder, label_csv,\n",
    "                                    transform_field=transform_field,\n",
    "                                    transform_draw=transform_draw)\n",
    "\n",
    "    field_folder = train_folder + \"field\"    # e.g., images of size 210x210\n",
    "    draw_folder = train_folder + \"rotated_draw2\"       # e.g., images of size 50x50\n",
    "    label_csv = train_folder + \"rotated_draw2/augmented_labels.csv\"           # CSV with columns: \"img_name\", \"x\", \"y\"\n",
    "\n",
    "    # Create dataset\n",
    "    dataset2 = CaptchaDetectionDataset(field_folder, draw_folder, label_csv,\n",
    "                                    transform_field=transform_field,\n",
    "                                    transform_draw=transform_draw)\n",
    "\n",
    "\n",
    "    dataset = ConcatDataset([dataset1, dataset2])\n",
    "    return dataset\n",
    "\n",
    "def create_test_dataset():\n",
    "    # Define transforms: Convert images to tensors, resize if needed, etc.\n",
    "    transform_field = transforms.Compose([\n",
    "        transforms.Resize((210, 210)),\n",
    "        transforms.ToTensor()  # Converts to tensor and scales pixel values to [0, 1]\n",
    "    ])\n",
    "    \n",
    "    transform_draw = transforms.Compose([\n",
    "        transforms.Resize((50, 50)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    test_folder = \"test_set/\"\n",
    "    # Folder paths\n",
    "    field_folder = test_folder + \"field\"   # e.g., images of size 210x210\n",
    "    draw_folder = test_folder + \"rotated_draw1\"       # e.g., images of size 50x50\n",
    "    label_csv = test_folder + \"rotated_draw1/augmented_labels.csv\"          # CSV with columns: \"img_name\", \"x\", \"y\"\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset1 = CaptchaDetectionDataset(field_folder, draw_folder, label_csv,\n",
    "                                    transform_field=transform_field,\n",
    "                                    transform_draw=transform_draw)\n",
    "\n",
    "    field_folder = test_folder + \"field\"    # e.g., images of size 210x210\n",
    "    draw_folder = test_folder + \"rotated_draw2\"       # e.g., images of size 50x50\n",
    "    label_csv = test_folder + \"rotated_draw2/augmented_labels.csv\"           # CSV with columns: \"img_name\", \"x\", \"y\"\n",
    "\n",
    "    # Create dataset\n",
    "    dataset2 = CaptchaDetectionDataset(field_folder, draw_folder, label_csv,\n",
    "                                    transform_field=transform_field,\n",
    "                                    transform_draw=transform_draw)\n",
    "\n",
    "\n",
    "    dataset = ConcatDataset([dataset1, dataset2])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = create_test_dataset()\n",
    "# dataset = create_train_dataset()\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Iterate over one batch and print shapes\n",
    "for field_img, draw_img, label in data_loader:\n",
    "    print(\"Field image batch shape:\", field_img.shape)  # Expected: (B, 3, 210, 210)\n",
    "    print(\"Draw image batch shape:\", draw_img.shape)    # Expected: (B, 3, 50, 50)\n",
    "    print(\"Label batch shape:\", label.shape)            # Expected: (B, 2)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted coordinates: tensor([[0.5612, 0.3785],\n",
      "        [0.5698, 0.4405],\n",
      "        [0.9280, 0.0444],\n",
      "        [0.3339, 0.2453]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DrawLocatorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DrawLocatorNet, self).__init__()\n",
    "        # Field branch: for the field image (210x210)\n",
    "        # We use three convolutional blocks.\n",
    "        self.field_cnn = nn.Sequential(\n",
    "            # Block 1: 210x210 -> 105x105\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # (B,32,210,210)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                           # (B,32,105,105)\n",
    "            \n",
    "            # Block 2: 105x105 -> 52x52 (approx.)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # (B,64,105,105)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                           # (B,64,52,52)\n",
    "            \n",
    "            # Block 3: 52x52 -> 26x26\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),# (B,128,52,52)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                            # (B,128,26,26)\n",
    "        )\n",
    "        \n",
    "        # Draw branch: for the draw image (50x50)\n",
    "        # We use three convolutional blocks and an adaptive average pooling to get a feature vector.\n",
    "        self.draw_cnn = nn.Sequential(\n",
    "            # Block 1: 50x50 -> 25x25\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),   # (B,32,50,50)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                              # (B,32,25,25)\n",
    "            \n",
    "            # Block 2: 25x25 -> 12x12 (approx.)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),   # (B,64,25,25)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                              # (B,64,12,12)\n",
    "            \n",
    "            # Block 3: 12x12 remains 12x12 but increases channels\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # (B,128,12,12)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))                    # (B,128,1,1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, field_img, draw_img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            field_img: Tensor of shape (B,3,210,210)\n",
    "            draw_img: Tensor of shape (B,3,50,50)\n",
    "        Returns:\n",
    "            preds: Tensor of shape (B,2) with the predicted (x, y) coordinates normalized in [0,1].\n",
    "        \"\"\"\n",
    "        # Extract features from the field image; shape: (B,128,26,26)\n",
    "        field_feat = self.field_cnn(field_img)\n",
    "        \n",
    "        # Extract feature vector from the draw image; shape: (B,128,1,1)\n",
    "        draw_feat = self.draw_cnn(draw_img)\n",
    "        draw_feat = draw_feat.view(draw_feat.size(0), -1)  # (B,128)\n",
    "        \n",
    "        # Compute the correlation map:\n",
    "        # For each spatial location in the field feature map, compute the dot product with the draw feature vector.\n",
    "        # Result: (B,26,26)\n",
    "        correlation = (field_feat * draw_feat.view(draw_feat.size(0), draw_feat.size(1), 1, 1)).sum(dim=1)\n",
    "        \n",
    "        # Get spatial dimensions (H, W) of the correlation map\n",
    "        B, H, W = correlation.size()\n",
    "        \n",
    "        # Create a coordinate grid corresponding to the correlation map.\n",
    "        device = correlation.device\n",
    "        grid_y, grid_x = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing='ij')\n",
    "        grid_x = grid_x.float()\n",
    "        grid_y = grid_y.float()\n",
    "        \n",
    "        # Flatten the correlation map and compute softmax to obtain a probability distribution over spatial locations.\n",
    "        correlation_flat = correlation.view(B, -1)  # shape: (B, H*W)\n",
    "        prob = F.softmax(correlation_flat, dim=1).view(B, H, W)  # shape: (B, H, W)\n",
    "        \n",
    "        # Compute the expected coordinates (soft-argmax) in the feature map grid.\n",
    "        pred_x = (prob * grid_x).view(B, -1).sum(dim=1)\n",
    "        pred_y = (prob * grid_y).view(B, -1).sum(dim=1)\n",
    "        \n",
    "        # The predicted coordinates are in the feature map scale (26x26).\n",
    "        # To normalize them to [0,1], we simply divide by the width/height of the feature map.\n",
    "        pred_x = pred_x / W\n",
    "        pred_y = pred_y / H\n",
    "        \n",
    "        preds = torch.stack([pred_x, pred_y], dim=1)  # shape: (B,2)\n",
    "        return preds\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    model = DrawLocatorNet()\n",
    "    # Create dummy inputs\n",
    "    field_img = torch.randn(4, 3, 210, 210)   # Batch of 4 field images\n",
    "    draw_img = torch.randn(4, 3, 50, 50)        # Batch of 4 draw images\n",
    "    \n",
    "    \n",
    "    preds = model(field_img, draw_img)\n",
    "    print(\"Predicted coordinates:\", preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TolerantMSELoss(nn.Module):\n",
    "    def __init__(self, epsilon=5.0, image_scale=210):\n",
    "        \"\"\"\n",
    "        Custom loss that does not penalize predictions within epsilon pixels (after standardization)\n",
    "        and applies MSE for errors outside that tolerance.\n",
    "        \n",
    "        Args:\n",
    "            epsilon (float): Tolerance in pixels.\n",
    "            image_scale (float): The size of the image dimension to normalize epsilon (e.g., 210).\n",
    "        \"\"\"\n",
    "        super(TolerantMSELoss, self).__init__()\n",
    "        # Convert pixel tolerance to normalized tolerance (0-1 range)\n",
    "        self.epsilon = epsilon / image_scale\n",
    "        self.mse = nn.MSELoss(reduction='none')  # We'll compute MSE per element\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions (Tensor): Predicted coordinates, shape (B,2).\n",
    "            targets (Tensor): Ground truth coordinates, shape (B,2).\n",
    "        Returns:\n",
    "            loss (Tensor): A scalar loss.\n",
    "        \"\"\"\n",
    "        # Compute element-wise squared error\n",
    "        abs_error = torch.abs(predictions - targets)  # shape: (B,2)\n",
    "\n",
    "        # Create a mask for errors less than or equal to tolerance on both coordinates\n",
    "        # We consider an instance \"correct\" if both x and y errors are within epsilon.\n",
    "        mask = (abs_error <= self.epsilon).all(dim=1)  # shape: (B,)\n",
    "\n",
    "        # For samples where the error is within tolerance, zero out the loss\n",
    "        loss_per_sample = self.mse(predictions, targets).sum(dim=1)  # MSE per sample (summing x and y)\n",
    "        loss_per_sample[mask] = 0.0\n",
    "\n",
    "        # Return the average loss over the batch\n",
    "        return loss_per_sample.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 13280.2716\n",
      "Epoch 1/10 | Val Loss: 13508.8804\n",
      "Epoch 2/10 | Train Loss: 13249.3255\n",
      "Epoch 2/10 | Val Loss: 13508.8823\n",
      "Epoch 3/10 | Train Loss: 13249.3255\n",
      "Epoch 3/10 | Val Loss: 13508.8881\n",
      "Epoch 4/10 | Train Loss: 13249.3255\n",
      "Epoch 4/10 | Val Loss: 13508.8944\n",
      "Epoch 5/10 | Train Loss: 13249.3255\n",
      "Epoch 5/10 | Val Loss: 13508.8805\n",
      "Epoch 6/10 | Train Loss: 13249.3255\n",
      "Epoch 6/10 | Val Loss: 13508.8751\n",
      "Epoch 7/10 | Train Loss: 13249.3256\n",
      "Epoch 7/10 | Val Loss: 13508.8674\n",
      "Epoch 8/10 | Train Loss: 13249.3255\n",
      "Epoch 8/10 | Val Loss: 13508.8796\n",
      "Epoch 9/10 | Train Loss: 13249.3255\n",
      "Epoch 9/10 | Val Loss: 13508.8902\n",
      "Epoch 10/10 | Train Loss: 13249.3255\n",
      "Epoch 10/10 | Val Loss: 13508.8705\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create train and test datasets\n",
    "train_dataset, test_dataset = create_train_dataset(), create_test_dataset()\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model (ensure DrawLocatorNet takes two inputs)\n",
    "model = DrawLocatorNet().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Coordinate regression loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 10\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for field_img, draw_img, labels in train_loader:\n",
    "        # Move data to GPU if available\n",
    "        field_img = field_img.to(device).float()\n",
    "        draw_img = draw_img.to(device).float()\n",
    "        labels = labels.to(device).float()  # Shape: (B,2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(field_img, draw_img)  # Model expects two inputs\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_train_loss = running_loss / len(train_loader)\n",
    "    train_loss.append(epoch_train_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for field_img, draw_img, labels in val_loader:\n",
    "            field_img = field_img.to(device).float()\n",
    "            draw_img = draw_img.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            outputs = model(field_img, draw_img)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(val_loader)\n",
    "    eval_loss.append(epoch_val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"draw_locator_net.pth\")\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, num_epochs+1), train_loss, label=\"Train Loss\")\n",
    "plt.plot(range(1, num_epochs+1), eval_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training & Validation Loss Curve\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premier perso\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'truncated_captchas/captcha_88.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 126\u001b[0m\n\u001b[0;32m    123\u001b[0m     predict_draw_location(model, field_path, draw_path)\n\u001b[0;32m    124\u001b[0m     draw_labels_on_image(field_path,labels_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncated_labels.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 126\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m88\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 117\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(captcha_num)\u001b[0m\n\u001b[0;32m    115\u001b[0m field_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncated_captchas/captcha_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaptcha_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m draw_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalized_premier_perso/captcha_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaptcha_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 117\u001b[0m \u001b[43mpredict_draw_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdraw_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m draw_labels_on_image(field_path,labels_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncated_labels.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSecond perso\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 34\u001b[0m, in \u001b[0;36mpredict_draw_location\u001b[1;34m(model, field_img_path, draw_img_path, device)\u001b[0m\n\u001b[0;32m     29\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     30\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),   \u001b[38;5;66;03m# Convert to tensor (C, H, W) and scale to [0,1]\u001b[39;00m\n\u001b[0;32m     31\u001b[0m ])\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Load and preprocess the images\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m field_img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield_img_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m210\u001b[39m, \u001b[38;5;241m210\u001b[39m))\n\u001b[0;32m     35\u001b[0m draw_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(draw_img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m))\n\u001b[0;32m     37\u001b[0m field_img \u001b[38;5;241m=\u001b[39m transform(field_img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Shape: (1, 3, 210, 210)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\PIL\\Image.py:3465\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3462\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[0;32m   3464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3465\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3466\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'truncated_captchas/captcha_88.png'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def predict_draw_location(model, field_img_path, draw_img_path, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Run inference on a given field image and draw image to predict the draw's location.\n",
    "\n",
    "    Args:\n",
    "        model: Trained DrawLocatorNet model.\n",
    "        field_img_path (str): Path to the field image (210x210).\n",
    "        draw_img_path (str): Path to the draw image (50x50).\n",
    "        device (str): Device to run inference on (\"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        (float, float): Predicted (x, y) coordinates in the field image (normalized 0-1 range).\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Define transforms (assuming normalization was applied during training)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),   # Convert to tensor (C, H, W) and scale to [0,1]\n",
    "    ])\n",
    "\n",
    "    # Load and preprocess the images\n",
    "    field_img = Image.open(field_img_path).convert('RGB').resize((210, 210))\n",
    "    draw_img = Image.open(draw_img_path).convert('RGB').resize((50, 50))\n",
    "\n",
    "    field_img = transform(field_img).unsqueeze(0).to(device)  # Shape: (1, 3, 210, 210)\n",
    "    draw_img = transform(draw_img).unsqueeze(0).to(device)    # Shape: (1, 3, 50, 50)\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        pred_coords = model(field_img, draw_img).cpu().numpy()[0]  # Shape: (2,)\n",
    "\n",
    "    # Extract and return the (x, y) coordinates\n",
    "    pred_x, pred_y = pred_coords*210\n",
    "    print(f\"Predicted Coordinates (Normalized): x={pred_x:.4f}, y={pred_y:.4f}\")\n",
    "    x = int(pred_x)\n",
    "    y = int(pred_y)\n",
    "    image_drawn = cv2.imread(field_img_path)\n",
    "    # image_drawn = image_resized.copy()\n",
    "    cv2.circle(image_drawn, (x, y), radius=5, color=(0, 0, 255), thickness=-1)  # Red circle\n",
    "\n",
    "    # Display the image with the drawn points\n",
    "    cv2.imshow(\"img\",image_drawn)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    return pred_x, pred_y\n",
    "\n",
    "def draw_labels_on_image(image_path, labels_file):\n",
    "    \"\"\"\n",
    "    Loads an image from image_path, retrieves ground truth (x1, y1, x2, y2) from labels.txt,\n",
    "    and displays the image with red circles at those coordinates.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image.\n",
    "        labels_file (str): Path to the labels file (CSV or TXT with x1, y1, x2, y2).\n",
    "    \"\"\"\n",
    "    # Load the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Unable to load image: {image_path}\")\n",
    "        return\n",
    "\n",
    "    # Resize the image to (width=340, height=410) to match the model's expected size\n",
    "    image_resized = image #cv2.resize(image, (340, 410))\n",
    "\n",
    "    # Read labels from the file\n",
    "    labels_df = pd.read_csv(labels_file)  # Ensure labels.txt is formatted correctly\n",
    "    image_name = image_path.split('/')[-1]  # Extract filename from path\n",
    "\n",
    "    # Find the row corresponding to the image name (assuming there is an 'id' or filename column)\n",
    "    if 'img_name' in labels_df.columns:\n",
    "        row = labels_df[labels_df['img_name'] == image_name]\n",
    "    else:\n",
    "        row = labels_df.iloc[0]  # If there's no ID column, just use the first row (for testing)\n",
    "\n",
    "    if row.empty:\n",
    "        print(f\"No labels found for {image_name}\")\n",
    "        return\n",
    "\n",
    "    # Extract ground truth coordinates\n",
    "    x1, y1, x2, y2 = row[['x1', 'y1', 'x2', 'y2']].to_numpy().flatten()\n",
    "    x1 = int(x1)\n",
    "    x2 = int(x2)\n",
    "    y1 = int(y1)\n",
    "    y2 = int(y2)\n",
    "    print(f\"Ground truth coordinates: x1={x1}, y1={y1}, x2={x2}, y2={y2}\")\n",
    "\n",
    "    # Draw red circles at the ground truth coordinates\n",
    "    image_drawn = image_resized.copy()\n",
    "    cv2.circle(image_drawn, (x1, y1), radius=5, color=(0, 255, 0), thickness=-1)  # Green circle\n",
    "    cv2.circle(image_drawn, (x2, y2), radius=5, color=(0, 0, 255), thickness=-1)  # Red circle\n",
    "\n",
    "    # Display the image with the drawn points\n",
    "    cv2.imshow(\"img\",image_drawn)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Load the trained model\n",
    "model = DrawLocatorNet()\n",
    "model.load_state_dict(torch.load(\"draw_locator_net.pth\", map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "def run(captcha_num):\n",
    "    print(\"Premier perso\")\n",
    "    field_path = f\"truncated_captchas/captcha_{captcha_num}.png\"\n",
    "    draw_path = f\"normalized_premier_perso/captcha_{captcha_num}.png\"\n",
    "    predict_draw_location(model, field_path, draw_path)\n",
    "    draw_labels_on_image(field_path,labels_file=\"truncated_labels.csv\")\n",
    "\n",
    "    print(\"Second perso\")\n",
    "    field_path = f\"truncated_captchas/captcha_{captcha_num}.png\"\n",
    "    draw_path = f\"normalized_second_perso/captcha_{captcha_num}.png\"\n",
    "    predict_draw_location(model, field_path, draw_path)\n",
    "    draw_labels_on_image(field_path,labels_file=\"truncated_labels.csv\")\n",
    "\n",
    "run(88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
