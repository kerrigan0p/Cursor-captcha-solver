{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field image batch shape: torch.Size([8, 3, 210, 210])\n",
      "Draw image batch shape: torch.Size([8, 3, 50, 50])\n",
      "Label batch shape: torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "class CaptchaDetectionDataset(Dataset):\n",
    "    def __init__(self, field_folder, draw_folder, label_csv, \n",
    "                 transform_field=None, transform_draw=None, transform_label=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            field_folder (str): Path to the folder containing field images (210x210).\n",
    "            draw_folder (str): Path to the folder containing draw images (50x50).\n",
    "            label_csv (str): Path to the CSV file containing labels with columns:\n",
    "                             \"img_name\", \"x\", \"y\".\n",
    "            transform_field (callable, optional): Transform to apply to field images.\n",
    "            transform_draw (callable, optional): Transform to apply to draw images.\n",
    "            transform_label (callable, optional): Transform to apply to labels.\n",
    "        \"\"\"\n",
    "        self.field_folder = field_folder\n",
    "        self.draw_folder = draw_folder\n",
    "        self.labels_df = pd.read_csv(label_csv)\n",
    "        \n",
    "        # Optional transforms for images/labels\n",
    "        self.transform_field = transform_field\n",
    "        self.transform_draw = transform_draw\n",
    "        self.transform_label = transform_label\n",
    "        \n",
    "        # Create a list of image names from the CSV\n",
    "        self.img_names = self.labels_df[\"img_name\"].tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image file name from CSV\n",
    "        img_name = self.img_names[idx]\n",
    "        \n",
    "        # Construct full paths for field and draw images\n",
    "        field_img_path = os.path.join(self.field_folder, img_name)\n",
    "        draw_img_path = os.path.join(self.draw_folder, img_name)\n",
    "        \n",
    "        # Load images using PIL and convert them to RGB\n",
    "        field_img = Image.open(field_img_path).convert('RGB')\n",
    "        draw_img = Image.open(draw_img_path).convert('RGB')\n",
    "        \n",
    "        # Apply transformations if provided (for example, resizing, normalization, converting to tensor)\n",
    "        if self.transform_field:\n",
    "            field_img = self.transform_field(field_img)\n",
    "        if self.transform_draw:\n",
    "            draw_img = self.transform_draw(draw_img)\n",
    "        \n",
    "        # Extract label (x, y) from the CSV row and convert to a float tensor\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        x = float(row[\"x\"])\n",
    "        y = float(row[\"y\"])\n",
    "        label = torch.tensor([x, y], dtype=torch.float32)\n",
    "        if self.transform_label:\n",
    "            label = self.transform_label(label)\n",
    "        \n",
    "        return field_img, draw_img, label\n",
    "\n",
    "# Example usage:\n",
    "def create_train_dataset():\n",
    "    # Define transforms: Convert images to tensors, resize if needed, etc.\n",
    "    transform_field = transforms.Compose([\n",
    "        transforms.Resize((210, 210)),\n",
    "        transforms.ToTensor()  # Converts to tensor and scales pixel values to [0, 1]\n",
    "    ])\n",
    "    \n",
    "    transform_draw = transforms.Compose([\n",
    "        transforms.Resize((50, 50)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_folder = \"train_set/\"\n",
    "    # Folder paths\n",
    "    field_folder = train_folder + \"field\"   # e.g., images of size 210x210\n",
    "    draw_folder = train_folder + \"rotated_draw1\"       # e.g., images of size 50x50\n",
    "    label_csv = train_folder + \"rotated_draw1/augmented_labels.csv\"          # CSV with columns: \"img_name\", \"x\", \"y\"\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset1 = CaptchaDetectionDataset(field_folder, draw_folder, label_csv,\n",
    "                                    transform_field=transform_field,\n",
    "                                    transform_draw=transform_draw)\n",
    "\n",
    "    field_folder = train_folder + \"field\"    # e.g., images of size 210x210\n",
    "    draw_folder = train_folder + \"rotated_draw2\"       # e.g., images of size 50x50\n",
    "    label_csv = train_folder + \"rotated_draw2/augmented_labels.csv\"           # CSV with columns: \"img_name\", \"x\", \"y\"\n",
    "\n",
    "    # Create dataset\n",
    "    dataset2 = CaptchaDetectionDataset(field_folder, draw_folder, label_csv,\n",
    "                                    transform_field=transform_field,\n",
    "                                    transform_draw=transform_draw)\n",
    "\n",
    "\n",
    "    dataset = ConcatDataset([dataset1, dataset2])\n",
    "    return dataset\n",
    "\n",
    "def create_test_dataset():\n",
    "    # Define transforms: Convert images to tensors, resize if needed, etc.\n",
    "    transform_field = transforms.Compose([\n",
    "        transforms.Resize((210, 210)),\n",
    "        transforms.ToTensor()  # Converts to tensor and scales pixel values to [0, 1]\n",
    "    ])\n",
    "    \n",
    "    transform_draw = transforms.Compose([\n",
    "        transforms.Resize((50, 50)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    test_folder = \"test_set/\"\n",
    "    # Folder paths\n",
    "    field_folder = test_folder + \"field\"   # e.g., images of size 210x210\n",
    "    draw_folder = test_folder + \"rotated_draw1\"       # e.g., images of size 50x50\n",
    "    label_csv = test_folder + \"rotated_draw1/augmented_labels.csv\"          # CSV with columns: \"img_name\", \"x\", \"y\"\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset1 = CaptchaDetectionDataset(field_folder, draw_folder, label_csv,\n",
    "                                    transform_field=transform_field,\n",
    "                                    transform_draw=transform_draw)\n",
    "\n",
    "    field_folder = test_folder + \"field\"    # e.g., images of size 210x210\n",
    "    draw_folder = test_folder + \"rotated_draw2\"       # e.g., images of size 50x50\n",
    "    label_csv = test_folder + \"rotated_draw2/augmented_labels.csv\"           # CSV with columns: \"img_name\", \"x\", \"y\"\n",
    "\n",
    "    # Create dataset\n",
    "    dataset2 = CaptchaDetectionDataset(field_folder, draw_folder, label_csv,\n",
    "                                    transform_field=transform_field,\n",
    "                                    transform_draw=transform_draw)\n",
    "\n",
    "\n",
    "    dataset = ConcatDataset([dataset1, dataset2])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = create_test_dataset()\n",
    "# dataset = create_train_dataset()\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Iterate over one batch and print shapes\n",
    "for field_img, draw_img, label in data_loader:\n",
    "    print(\"Field image batch shape:\", field_img.shape)  # Expected: (B, 3, 210, 210)\n",
    "    print(\"Draw image batch shape:\", draw_img.shape)    # Expected: (B, 3, 50, 50)\n",
    "    print(\"Label batch shape:\", label.shape)            # Expected: (B, 2)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted coordinates: tensor([[0.4026, 0.3258],\n",
      "        [0.0789, 0.2588],\n",
      "        [0.4565, 0.1649],\n",
      "        [0.0200, 0.5286]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DrawLocatorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DrawLocatorNet, self).__init__()\n",
    "        # Field branch: for the field image (210x210)\n",
    "        # We use three convolutional blocks.\n",
    "        self.field_cnn = nn.Sequential(\n",
    "            # Block 1: 210x210 -> 105x105\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # (B,32,210,210)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                           # (B,32,105,105)\n",
    "            \n",
    "            # Block 2: 105x105 -> 52x52 (approx.)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # (B,64,105,105)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                           # (B,64,52,52)\n",
    "            \n",
    "            # Block 3: 52x52 -> 26x26\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),# (B,128,52,52)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                            # (B,128,26,26)\n",
    "        )\n",
    "        \n",
    "        # Draw branch: for the draw image (50x50)\n",
    "        # We use three convolutional blocks and an adaptive average pooling to get a feature vector.\n",
    "        self.draw_cnn = nn.Sequential(\n",
    "            # Block 1: 50x50 -> 25x25\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),   # (B,32,50,50)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                              # (B,32,25,25)\n",
    "            \n",
    "            # Block 2: 25x25 -> 12x12 (approx.)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),   # (B,64,25,25)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                              # (B,64,12,12)\n",
    "            \n",
    "            # Block 3: 12x12 remains 12x12 but increases channels\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # (B,128,12,12)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))                    # (B,128,1,1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, field_img, draw_img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            field_img: Tensor of shape (B,3,210,210)\n",
    "            draw_img: Tensor of shape (B,3,50,50)\n",
    "        Returns:\n",
    "            preds: Tensor of shape (B,2) with the predicted (x, y) coordinates normalized in [0,1].\n",
    "        \"\"\"\n",
    "        # Extract features from the field image; shape: (B,128,26,26)\n",
    "        field_feat = self.field_cnn(field_img)\n",
    "        \n",
    "        # Extract feature vector from the draw image; shape: (B,128,1,1)\n",
    "        draw_feat = self.draw_cnn(draw_img)\n",
    "        draw_feat = draw_feat.view(draw_feat.size(0), -1)  # (B,128)\n",
    "        \n",
    "        # Compute the correlation map:\n",
    "        # For each spatial location in the field feature map, compute the dot product with the draw feature vector.\n",
    "        # Result: (B,26,26)\n",
    "        correlation = (field_feat * draw_feat.view(draw_feat.size(0), draw_feat.size(1), 1, 1)).sum(dim=1)\n",
    "        \n",
    "        # Get spatial dimensions (H, W) of the correlation map\n",
    "        B, H, W = correlation.size()\n",
    "        \n",
    "        # Create a coordinate grid corresponding to the correlation map.\n",
    "        device = correlation.device\n",
    "        grid_y, grid_x = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing='ij')\n",
    "        grid_x = grid_x.float()\n",
    "        grid_y = grid_y.float()\n",
    "        \n",
    "        # Flatten the correlation map and compute softmax to obtain a probability distribution over spatial locations.\n",
    "        correlation_flat = correlation.view(B, -1)  # shape: (B, H*W)\n",
    "        prob = F.softmax(correlation_flat, dim=1).view(B, H, W)  # shape: (B, H, W)\n",
    "        \n",
    "        # Compute the expected coordinates (soft-argmax) in the feature map grid.\n",
    "        pred_x = (prob * grid_x).view(B, -1).sum(dim=1)\n",
    "        pred_y = (prob * grid_y).view(B, -1).sum(dim=1)\n",
    "        \n",
    "        # The predicted coordinates are in the feature map scale (26x26).\n",
    "        # To normalize them to [0,1], we simply divide by the width/height of the feature map.\n",
    "        pred_x = pred_x / W\n",
    "        pred_y = pred_y / H\n",
    "        \n",
    "        preds = torch.stack([pred_x, pred_y], dim=1)  # shape: (B,2)\n",
    "        return preds\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    model = DrawLocatorNet()\n",
    "    # Create dummy inputs\n",
    "    field_img = torch.randn(4, 3, 210, 210)   # Batch of 4 field images\n",
    "    draw_img = torch.randn(4, 3, 50, 50)        # Batch of 4 draw images\n",
    "    \n",
    "    \n",
    "    preds = model(field_img, draw_img)\n",
    "    print(\"Predicted coordinates:\", preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TolerantMSELoss(nn.Module):\n",
    "    def __init__(self, epsilon=3.0, image_scale=210):\n",
    "        \"\"\"\n",
    "        Custom loss that does not penalize predictions within epsilon pixels (after standardization)\n",
    "        and applies MSE for errors outside that tolerance.\n",
    "        \n",
    "        Args:\n",
    "            epsilon (float): Tolerance in pixels.\n",
    "            image_scale (float): The size of the image dimension to normalize epsilon (e.g., 210).\n",
    "        \"\"\"\n",
    "        super(TolerantMSELoss, self).__init__()\n",
    "        # Convert pixel tolerance to normalized tolerance (0-1 range)\n",
    "        self.epsilon = epsilon / image_scale\n",
    "        self.mse = nn.MSELoss(reduction='none')  # We'll compute MSE per element\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions (Tensor): Predicted coordinates, shape (B,2).\n",
    "            targets (Tensor): Ground truth coordinates, shape (B,2).\n",
    "        Returns:\n",
    "            loss (Tensor): A scalar loss.\n",
    "        \"\"\"\n",
    "        # Compute element-wise squared error\n",
    "        abs_error = torch.abs(predictions - targets)  # shape: (B,2)\n",
    "\n",
    "        # Create a mask for errors less than or equal to tolerance on both coordinates\n",
    "        # We consider an instance \"correct\" if both x and y errors are within epsilon.\n",
    "        mask = (abs_error <= self.epsilon).all(dim=1)  # shape: (B,)\n",
    "\n",
    "        # For samples where the error is within tolerance, zero out the loss\n",
    "        loss_per_sample = self.mse(predictions, targets).sum(dim=1)  # MSE per sample (summing x and y)\n",
    "        loss_per_sample[mask] = 0.0\n",
    "\n",
    "        # Return the average loss over the batch\n",
    "        return loss_per_sample.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 0.1267\n",
      "Epoch 1/100 | Val Loss: 0.1168\n",
      "Epoch 2/100 | Train Loss: 0.1056\n",
      "Epoch 2/100 | Val Loss: 0.0640\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 102\u001b[0m\n\u001b[0;32m     98\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining & Validation Loss Curve\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     99\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m--> 102\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmilestones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 48\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(num_epochs, milestones, gamma)\u001b[0m\n\u001b[0;32m     45\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(field_img, draw_img)  \u001b[38;5;66;03m# Model expects two inputs\u001b[39;00m\n\u001b[0;32m     46\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 48\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     50\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(num_epochs = 10,milestones = [1], gamma = 0.5):\n",
    "    # Create train and test datasets\n",
    "    train_dataset, test_dataset = create_train_dataset(), create_test_dataset()\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize the model (ensure DrawLocatorNet takes two inputs)\n",
    "    model = DrawLocatorNet().to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()  # Coordinate regression loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "\n",
    "    train_loss = []\n",
    "    eval_loss = []\n",
    "\n",
    "    model_name = f\"e{num_epochs}_g{100*gamma}.pth\"\n",
    "    # Open log file\n",
    "    log_file = open(\"training_logs.txt\", \"w\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch_idx, (field_img, draw_img, labels) in enumerate(train_loader):\n",
    "            # Move data to GPU if available\n",
    "            field_img = field_img.to(device).float()\n",
    "            draw_img = draw_img.to(device).float()\n",
    "            labels = labels.to(device).float()  # Shape: (B,2)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(field_img, draw_img)  # Model expects two inputs\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Log predictions and actual labels\n",
    "            for i in range(len(labels)):\n",
    "                log_file.write(f\"Epoch: {epoch+1}, Batch: {batch_idx}, Train Prediction: {outputs[i].cpu().tolist()}, Actual: {labels[i].cpu().tolist()}\\n\")\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_loader)\n",
    "        train_loss.append(epoch_train_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_train_loss:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (field_img, draw_img, labels) in enumerate(val_loader):\n",
    "                field_img = field_img.to(device).float()\n",
    "                draw_img = draw_img.to(device).float()\n",
    "                labels = labels.to(device).float()\n",
    "\n",
    "                outputs = model(field_img, draw_img)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                # Log validation predictions and actual labels\n",
    "                for i in range(len(labels)):\n",
    "                    log_file.write(f\"Epoch: {epoch+1}, Batch: {batch_idx}, Val Prediction: {outputs[i].cpu().tolist()}, Actual: {labels[i].cpu().tolist()}\\n\")\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(val_loader)\n",
    "        eval_loss.append(epoch_val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Val Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "        # Flush log file to ensure data is written\n",
    "        log_file.flush()\n",
    "        scheduler.step()  # Update learning rate\n",
    "        torch.save(model.state_dict(), f\"e{num_epochs}_g{100*gamma}_{epoch}.pth\")\n",
    "    # Close log file\n",
    "    log_file.close()\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "\n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, num_epochs+1), train_loss, label=\"Train Loss\")\n",
    "    plt.plot(range(1, num_epochs+1), eval_loss, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training & Validation Loss Curve\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "train(num_epochs=100,milestones=[15,25,40,50,60,70,80,90],gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premier perso\n",
      "Predicted Coordinates (Normalized): x=140.2265, y=170.0038\n",
      "Unable to load image: captcha_87.png\n",
      "Second perso\n",
      "Predicted Coordinates (Normalized): x=153.8066, y=110.3225\n",
      "Unable to load image: captcha_87.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def predict_draw_location(model, field_img_path, draw_img_path, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Run inference on a given field image and draw image to predict the draw's location.\n",
    "\n",
    "    Args:\n",
    "        model: Trained DrawLocatorNet model.\n",
    "        field_img_path (str): Path to the field image (210x210).\n",
    "        draw_img_path (str): Path to the draw image (50x50).\n",
    "        device (str): Device to run inference on (\"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        (float, float): Predicted (x, y) coordinates in the field image (normalized 0-1 range).\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Define transforms (assuming normalization was applied during training)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),   # Convert to tensor (C, H, W) and scale to [0,1]\n",
    "    ])\n",
    "\n",
    "    # Load and preprocess the images\n",
    "    field_img = Image.open(field_img_path).convert('RGB').resize((210, 210))\n",
    "    draw_img = Image.open(draw_img_path).convert('RGB').resize((50, 50))\n",
    "\n",
    "    field_img = transform(field_img).unsqueeze(0).to(device)  # Shape: (1, 3, 210, 210)\n",
    "    draw_img = transform(draw_img).unsqueeze(0).to(device)    # Shape: (1, 3, 50, 50)\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        pred_coords = model(field_img, draw_img).cpu().numpy()[0]  # Shape: (2,)\n",
    "\n",
    "    # Extract and return the (x, y) coordinates\n",
    "    pred_x, pred_y = pred_coords*210\n",
    "    print(f\"Predicted Coordinates (Normalized): x={pred_x:.4f}, y={pred_y:.4f}\")\n",
    "    x = int(pred_x)\n",
    "    y = int(pred_y)\n",
    "    image_drawn = cv2.imread(field_img_path)\n",
    "    # image_drawn = image_resized.copy()\n",
    "    cv2.circle(image_drawn, (x, y), radius=5, color=(0, 0, 255), thickness=-1)  # Red circle\n",
    "\n",
    "    # Display the image with the drawn points\n",
    "    cv2.imshow(\"img\",image_drawn)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def draw_labels_on_image(image_path, labels_file):\n",
    "    \"\"\"\n",
    "    Loads an image from image_path, retrieves ground truth (x1, y1, x2, y2) from labels.txt,\n",
    "    and displays the image with red circles at those coordinates.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image.\n",
    "        labels_file (str): Path to the labels file (CSV or TXT with x1, y1, x2, y2).\n",
    "    \"\"\"\n",
    "    # Load the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Unable to load image: {image_path}\")\n",
    "        return\n",
    "\n",
    "    # Resize the image to (width=340, height=410) to match the model's expected size\n",
    "    image_resized = image #cv2.resize(image, (340, 410))\n",
    "\n",
    "    # Read labels from the file\n",
    "    labels_df = pd.read_csv(labels_file)  # Ensure labels.txt is formatted correctly\n",
    "    image_name = image_path.split('/')[-1]  # Extract filename from path\n",
    "\n",
    "    # Find the row corresponding to the image name (assuming there is an 'id' or filename column)\n",
    "    if 'img_name' in labels_df.columns:\n",
    "        row = labels_df[labels_df['img_name'] == image_name]\n",
    "    else:\n",
    "        row = labels_df.iloc[0]  # If there's no ID column, just use the first row (for testing)\n",
    "\n",
    "    if row.empty:\n",
    "        print(f\"No labels found for {image_name}\")\n",
    "        return\n",
    "\n",
    "    # Extract ground truth coordinates\n",
    "    x1, y1, x2, y2 = row[['x1', 'y1', 'x2', 'y2']].to_numpy().flatten()\n",
    "    x1 = int(x1)\n",
    "    x2 = int(x2)\n",
    "    y1 = int(y1)\n",
    "    y2 = int(y2)\n",
    "    print(f\"Ground truth coordinates: x1={x1}, y1={y1}, x2={x2}, y2={y2}\")\n",
    "\n",
    "    # Draw red circles at the ground truth coordinates\n",
    "    image_drawn = image_resized.copy()\n",
    "    cv2.circle(image_drawn, (x1, y1), radius=5, color=(0, 255, 0), thickness=-1)  # Green circle\n",
    "    cv2.circle(image_drawn, (x2, y2), radius=5, color=(0, 0, 255), thickness=-1)  # Red circle\n",
    "\n",
    "    # Display the image with the drawn points\n",
    "    cv2.imshow(\"img\",image_drawn)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Load the trained model\n",
    "model = DrawLocatorNet()\n",
    "model.load_state_dict(torch.load(\"e100_m20_g50.pth\", map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "def run(captcha_num):\n",
    "    print(\"Premier perso\")\n",
    "    captcha_name = f\"captcha_{captcha_num}.png\"\n",
    "    field_path = f\"test_set/field/captcha_{captcha_num}_rotated_2.png\"\n",
    "    draw_path = f\"test_set/rotated_draw1/captcha_{captcha_num}_rotated_2.png\" #captcha_5_rotated_2\n",
    "    label_path = f\"truncated_labels.csv\"\n",
    "    predict_draw_location(model, field_path, draw_path)\n",
    "    draw_labels_on_image(captcha_name,labels_file=label_path)\n",
    "\n",
    "    print(\"Second perso\")\n",
    "    field_path = f\"test_set/field/captcha_{captcha_num}_rotated_2.png\"\n",
    "    draw_path = f\"test_set/rotated_draw2/captcha_{captcha_num}_rotated_2.png\" #captcha_5_rotated_2\n",
    "    predict_draw_location(model, field_path, draw_path)\n",
    "    draw_labels_on_image(captcha_name,labels_file=label_path)\n",
    "\n",
    "run(87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
