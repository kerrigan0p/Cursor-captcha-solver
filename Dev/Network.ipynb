{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field image batch shape: torch.Size([8, 3, 210, 210])\n",
      "Draw image batch shape: torch.Size([8, 3, 50, 50])\n",
      "Label batch shape: torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "\n",
    "class CaptchaDetectionDataset(Dataset):\n",
    "    def __init__(self, field_folder, draw_folder, label_csv, \n",
    "                 transform_field=None, transform_draw=None, transform_label=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            field_folder (str): Path to the folder containing field images (210x210).\n",
    "            draw_folder (str): Path to the folder containing draw images (50x50).\n",
    "            label_csv (str): Path to the CSV file containing labels with columns:\n",
    "                             \"img_name\", \"x\", \"y\".\n",
    "            transform_field (callable, optional): Transform to apply to field images.\n",
    "            transform_draw (callable, optional): Transform to apply to draw images.\n",
    "            transform_label (callable, optional): Transform to apply to labels.\n",
    "        \"\"\"\n",
    "        self.field_folder = field_folder\n",
    "        self.draw_folder = draw_folder\n",
    "        self.labels_df = pd.read_csv(label_csv)\n",
    "        \n",
    "        # Optional transforms for images/labels\n",
    "        self.transform_field = transform_field\n",
    "        self.transform_draw = transform_draw\n",
    "        self.transform_label = transform_label\n",
    "        \n",
    "        # Create a list of image names from the CSV\n",
    "        self.img_names = self.labels_df[\"img_name\"].tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image file name from CSV\n",
    "        img_name = self.img_names[idx]\n",
    "        \n",
    "        # Construct full paths for field and draw images\n",
    "        field_img_path = os.path.join(self.field_folder, img_name)\n",
    "        draw_img_path = os.path.join(self.draw_folder, img_name)\n",
    "        \n",
    "        # Load images using PIL and convert them to RGB\n",
    "        field_img = Image.open(field_img_path).convert('RGB')\n",
    "        draw_img = Image.open(draw_img_path).convert('RGB')\n",
    "        \n",
    "        # Apply transformations if provided (for example, resizing, normalization, converting to tensor)\n",
    "        if self.transform_field:\n",
    "            field_img = self.transform_field(field_img)\n",
    "        if self.transform_draw:\n",
    "            draw_img = self.transform_draw(draw_img)\n",
    "        \n",
    "        # Extract label (x, y) from the CSV row and convert to a float tensor\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        x = float(row[\"x\"])\n",
    "        y = float(row[\"y\"])\n",
    "        label = torch.tensor([x, y], dtype=torch.float32)\n",
    "        if self.transform_label:\n",
    "            label = self.transform_label(label)\n",
    "        \n",
    "        return field_img, draw_img, label\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    # Define transforms: Convert images to tensors, resize if needed, etc.\n",
    "    transform_field = transforms.Compose([\n",
    "        transforms.Resize((210, 210)),\n",
    "        transforms.ToTensor()  # Converts to tensor and scales pixel values to [0, 1]\n",
    "    ])\n",
    "    \n",
    "    transform_draw = transforms.Compose([\n",
    "        transforms.Resize((50, 50)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Folder paths\n",
    "    field_folder = \"normalized_images\"   # e.g., images of size 210x210\n",
    "    draw_folder = \"normalized_premier_perso\"       # e.g., images of size 50x50\n",
    "    label_csv = \"labels.csv\"          # CSV with columns: \"img_name\", \"x\", \"y\"\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = CaptchaDetectionDataset(field_folder, draw_folder, label_csv,\n",
    "                                      transform_field=transform_field,\n",
    "                                      transform_draw=transform_draw)\n",
    "    \n",
    "    # Create DataLoader for batching and shuffling\n",
    "    from torch.utils.data import DataLoader\n",
    "    data_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "    \n",
    "    # Iterate over one batch and print shapes\n",
    "    for field_img, draw_img, label in data_loader:\n",
    "        print(\"Field image batch shape:\", field_img.shape)  # Expected: (B, 3, 210, 210)\n",
    "        print(\"Draw image batch shape:\", draw_img.shape)    # Expected: (B, 3, 50, 50)\n",
    "        print(\"Label batch shape:\", label.shape)            # Expected: (B, 2)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted coordinates: tensor([[0.4154, 0.8589],\n",
      "        [0.3797, 0.9452],\n",
      "        [0.3776, 0.6773],\n",
      "        [0.3170, 0.8596]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DrawLocatorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DrawLocatorNet, self).__init__()\n",
    "        # Field branch: for the field image (210x210)\n",
    "        # We use three convolutional blocks.\n",
    "        self.field_cnn = nn.Sequential(\n",
    "            # Block 1: 210x210 -> 105x105\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # (B,32,210,210)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                           # (B,32,105,105)\n",
    "            \n",
    "            # Block 2: 105x105 -> 52x52 (approx.)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # (B,64,105,105)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                           # (B,64,52,52)\n",
    "            \n",
    "            # Block 3: 52x52 -> 26x26\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),# (B,128,52,52)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                            # (B,128,26,26)\n",
    "        )\n",
    "        \n",
    "        # Draw branch: for the draw image (50x50)\n",
    "        # We use three convolutional blocks and an adaptive average pooling to get a feature vector.\n",
    "        self.draw_cnn = nn.Sequential(\n",
    "            # Block 1: 50x50 -> 25x25\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),   # (B,32,50,50)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                              # (B,32,25,25)\n",
    "            \n",
    "            # Block 2: 25x25 -> 12x12 (approx.)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),   # (B,64,25,25)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                              # (B,64,12,12)\n",
    "            \n",
    "            # Block 3: 12x12 remains 12x12 but increases channels\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # (B,128,12,12)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))                    # (B,128,1,1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, field_img, draw_img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            field_img: Tensor of shape (B,3,210,210)\n",
    "            draw_img: Tensor of shape (B,3,50,50)\n",
    "        Returns:\n",
    "            preds: Tensor of shape (B,2) with the predicted (x, y) coordinates normalized in [0,1].\n",
    "        \"\"\"\n",
    "        # Extract features from the field image; shape: (B,128,26,26)\n",
    "        field_feat = self.field_cnn(field_img)\n",
    "        \n",
    "        # Extract feature vector from the draw image; shape: (B,128,1,1)\n",
    "        draw_feat = self.draw_cnn(draw_img)\n",
    "        draw_feat = draw_feat.view(draw_feat.size(0), -1)  # (B,128)\n",
    "        \n",
    "        # Compute the correlation map:\n",
    "        # For each spatial location in the field feature map, compute the dot product with the draw feature vector.\n",
    "        # Result: (B,26,26)\n",
    "        correlation = (field_feat * draw_feat.view(draw_feat.size(0), draw_feat.size(1), 1, 1)).sum(dim=1)\n",
    "        \n",
    "        # Get spatial dimensions (H, W) of the correlation map\n",
    "        B, H, W = correlation.size()\n",
    "        \n",
    "        # Create a coordinate grid corresponding to the correlation map.\n",
    "        device = correlation.device\n",
    "        grid_y, grid_x = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing='ij')\n",
    "        grid_x = grid_x.float()\n",
    "        grid_y = grid_y.float()\n",
    "        \n",
    "        # Flatten the correlation map and compute softmax to obtain a probability distribution over spatial locations.\n",
    "        correlation_flat = correlation.view(B, -1)  # shape: (B, H*W)\n",
    "        prob = F.softmax(correlation_flat, dim=1).view(B, H, W)  # shape: (B, H, W)\n",
    "        \n",
    "        # Compute the expected coordinates (soft-argmax) in the feature map grid.\n",
    "        pred_x = (prob * grid_x).view(B, -1).sum(dim=1)\n",
    "        pred_y = (prob * grid_y).view(B, -1).sum(dim=1)\n",
    "        \n",
    "        # The predicted coordinates are in the feature map scale (26x26).\n",
    "        # To normalize them to [0,1], we simply divide by the width/height of the feature map.\n",
    "        pred_x = pred_x / W\n",
    "        pred_y = pred_y / H\n",
    "        \n",
    "        preds = torch.stack([pred_x, pred_y], dim=1)  # shape: (B,2)\n",
    "        return preds\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    model = DrawLocatorNet()\n",
    "    # Create dummy inputs\n",
    "    field_img = torch.randn(4, 3, 210, 210)   # Batch of 4 field images\n",
    "    draw_img = torch.randn(4, 3, 50, 50)        # Batch of 4 draw images\n",
    "    \n",
    "    \n",
    "    preds = model(field_img, draw_img)\n",
    "    print(\"Predicted coordinates:\", preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TolerantMSELoss(nn.Module):\n",
    "    def __init__(self, epsilon=5.0, image_scale=210):\n",
    "        \"\"\"\n",
    "        Custom loss that does not penalize predictions within epsilon pixels (after standardization)\n",
    "        and applies MSE for errors outside that tolerance.\n",
    "        \n",
    "        Args:\n",
    "            epsilon (float): Tolerance in pixels.\n",
    "            image_scale (float): The size of the image dimension to normalize epsilon (e.g., 210).\n",
    "        \"\"\"\n",
    "        super(TolerantMSELoss, self).__init__()\n",
    "        # Convert pixel tolerance to normalized tolerance (0-1 range)\n",
    "        self.epsilon = epsilon / image_scale\n",
    "        self.mse = nn.MSELoss(reduction='none')  # We'll compute MSE per element\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions (Tensor): Predicted coordinates, shape (B,2).\n",
    "            targets (Tensor): Ground truth coordinates, shape (B,2).\n",
    "        Returns:\n",
    "            loss (Tensor): A scalar loss.\n",
    "        \"\"\"\n",
    "        # Compute element-wise squared error\n",
    "        abs_error = torch.abs(predictions - targets)  # shape: (B,2)\n",
    "\n",
    "        # Create a mask for errors less than or equal to tolerance on both coordinates\n",
    "        # We consider an instance \"correct\" if both x and y errors are within epsilon.\n",
    "        mask = (abs_error <= self.epsilon).all(dim=1)  # shape: (B,)\n",
    "\n",
    "        # For samples where the error is within tolerance, zero out the loss\n",
    "        loss_per_sample = self.mse(predictions, targets).sum(dim=1)  # MSE per sample (summing x and y)\n",
    "        loss_per_sample[mask] = 0.0\n",
    "\n",
    "        # Return the average loss over the batch\n",
    "        return loss_per_sample.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.1232\n",
      "Epoch 1/10, Validation Loss: 0.0801\n",
      "Epoch 2/10, Training Loss: 0.0980\n",
      "Epoch 2/10, Validation Loss: 0.1013\n",
      "Epoch 3/10, Training Loss: 0.0853\n",
      "Epoch 3/10, Validation Loss: 0.0934\n",
      "Epoch 4/10, Training Loss: 0.0782\n",
      "Epoch 4/10, Validation Loss: 0.1034\n",
      "Epoch 5/10, Training Loss: 0.0716\n",
      "Epoch 5/10, Validation Loss: 0.0952\n",
      "Epoch 6/10, Training Loss: 0.0688\n",
      "Epoch 6/10, Validation Loss: 0.0936\n",
      "Epoch 7/10, Training Loss: 0.0679\n",
      "Epoch 7/10, Validation Loss: 0.0942\n",
      "Epoch 8/10, Training Loss: 0.0668\n",
      "Epoch 8/10, Validation Loss: 0.0998\n",
      "Epoch 9/10, Training Loss: 0.0659\n",
      "Epoch 9/10, Validation Loss: 0.0974\n",
      "Epoch 10/10, Training Loss: 0.0666\n",
      "Epoch 10/10, Validation Loss: 0.0963\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "# Define train-test split ratio\n",
    "train_ratio = 0.8  # 80% train, 20% test\n",
    "test_ratio = 1 - train_ratio\n",
    "\n",
    "# Compute sizes\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "test_size = dataset_size - train_size  # Ensures all samples are used\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_indices = train_dataset.indices\n",
    "test_indices = test_dataset.indices\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model (using the DrawLocatorNet defined previously)\n",
    "model = DrawLocatorNet().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Since we're doing coordinate regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 10\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for field_img, draw_img, labels in train_loader:\n",
    "        # Move data to device and ensure dtype is float32\n",
    "        field_img = field_img.to(device).float()\n",
    "        draw_img = draw_img.to(device).float()\n",
    "        labels = labels.to(device).float()  # Expected shape: (B,2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(field_img, draw_img)  # Expected shape: (B, 2)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * field_img.size(0)\n",
    "\n",
    "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_loss.append(epoch_train_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for field_img, draw_img, labels in val_loader:\n",
    "            field_img = field_img.to(device).float()\n",
    "            draw_img = draw_img.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            outputs = model(field_img, draw_img)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item() * field_img.size(0)\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "    eval_loss.append(epoch_val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "# Optionally, save the trained model\n",
    "torch.save(model.state_dict(), \"draw_locator_net.pth\")\n",
    "print(train_indices)\n",
    "print(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premier perso\n",
      "Predicted Coordinates (Normalized): x=56.5478, y=96.9207\n",
      "Ground truth coordinates: x1=103, y1=72, x2=35, y2=100\n",
      "Second perso\n",
      "Predicted Coordinates (Normalized): x=56.5501, y=96.9198\n",
      "Ground truth coordinates: x1=103, y1=72, x2=35, y2=100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def predict_draw_location(model, field_img_path, draw_img_path, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Run inference on a given field image and draw image to predict the draw's location.\n",
    "\n",
    "    Args:\n",
    "        model: Trained DrawLocatorNet model.\n",
    "        field_img_path (str): Path to the field image (210x210).\n",
    "        draw_img_path (str): Path to the draw image (50x50).\n",
    "        device (str): Device to run inference on (\"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        (float, float): Predicted (x, y) coordinates in the field image (normalized 0-1 range).\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Define transforms (assuming normalization was applied during training)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),   # Convert to tensor (C, H, W) and scale to [0,1]\n",
    "    ])\n",
    "\n",
    "    # Load and preprocess the images\n",
    "    field_img = Image.open(field_img_path).convert('RGB').resize((210, 210))\n",
    "    draw_img = Image.open(draw_img_path).convert('RGB').resize((50, 50))\n",
    "\n",
    "    field_img = transform(field_img).unsqueeze(0).to(device)  # Shape: (1, 3, 210, 210)\n",
    "    draw_img = transform(draw_img).unsqueeze(0).to(device)    # Shape: (1, 3, 50, 50)\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        pred_coords = model(field_img, draw_img).cpu().numpy()[0]  # Shape: (2,)\n",
    "\n",
    "    # Extract and return the (x, y) coordinates\n",
    "    pred_x, pred_y = pred_coords*210\n",
    "    print(f\"Predicted Coordinates (Normalized): x={pred_x:.4f}, y={pred_y:.4f}\")\n",
    "    x = int(pred_x)\n",
    "    y = int(pred_y)\n",
    "    image_drawn = cv2.imread(field_img_path)\n",
    "    # image_drawn = image_resized.copy()\n",
    "    cv2.circle(image_drawn, (x, y), radius=5, color=(0, 0, 255), thickness=-1)  # Red circle\n",
    "\n",
    "    # Display the image with the drawn points\n",
    "    cv2.imshow(\"img\",image_drawn)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    return pred_x, pred_y\n",
    "\n",
    "def draw_labels_on_image(image_path, labels_file):\n",
    "    \"\"\"\n",
    "    Loads an image from image_path, retrieves ground truth (x1, y1, x2, y2) from labels.txt,\n",
    "    and displays the image with red circles at those coordinates.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image.\n",
    "        labels_file (str): Path to the labels file (CSV or TXT with x1, y1, x2, y2).\n",
    "    \"\"\"\n",
    "    # Load the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Unable to load image: {image_path}\")\n",
    "        return\n",
    "\n",
    "    # Resize the image to (width=340, height=410) to match the model's expected size\n",
    "    image_resized = image #cv2.resize(image, (340, 410))\n",
    "\n",
    "    # Read labels from the file\n",
    "    labels_df = pd.read_csv(labels_file)  # Ensure labels.txt is formatted correctly\n",
    "    image_name = image_path.split('/')[-1]  # Extract filename from path\n",
    "\n",
    "    # Find the row corresponding to the image name (assuming there is an 'id' or filename column)\n",
    "    if 'img_name' in labels_df.columns:\n",
    "        row = labels_df[labels_df['img_name'] == image_name]\n",
    "    else:\n",
    "        row = labels_df.iloc[0]  # If there's no ID column, just use the first row (for testing)\n",
    "\n",
    "    if row.empty:\n",
    "        print(f\"No labels found for {image_name}\")\n",
    "        return\n",
    "\n",
    "    # Extract ground truth coordinates\n",
    "    x1, y1, x2, y2 = row[['x1', 'y1', 'x2', 'y2']].to_numpy().flatten()\n",
    "    x1 = int(x1)\n",
    "    x2 = int(x2)\n",
    "    y1 = int(y1)\n",
    "    y2 = int(y2)\n",
    "    print(f\"Ground truth coordinates: x1={x1}, y1={y1}, x2={x2}, y2={y2}\")\n",
    "\n",
    "    # Draw red circles at the ground truth coordinates\n",
    "    image_drawn = image_resized.copy()\n",
    "    cv2.circle(image_drawn, (x1, y1), radius=5, color=(0, 255, 0), thickness=-1)  # Green circle\n",
    "    cv2.circle(image_drawn, (x2, y2), radius=5, color=(0, 0, 255), thickness=-1)  # Red circle\n",
    "\n",
    "    # Display the image with the drawn points\n",
    "    cv2.imshow(\"img\",image_drawn)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Load the trained model\n",
    "model = DrawLocatorNet()\n",
    "model.load_state_dict(torch.load(\"draw_locator_net.pth\", map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "def run(captcha_num):\n",
    "    print(\"Premier perso\")\n",
    "    field_path = f\"truncated_captchas/captcha_{captcha_num}.png\"\n",
    "    draw_path = f\"normalized_premier_perso/captcha_{captcha_num}.png\"\n",
    "    predict_draw_location(model, field_path, draw_path)\n",
    "    draw_labels_on_image(field_path,labels_file=\"truncated_labels.csv\")\n",
    "\n",
    "    print(\"Second perso\")\n",
    "    field_path = f\"truncated_captchas/captcha_{captcha_num}.png\"\n",
    "    draw_path = f\"normalized_second_perso/captcha_{captcha_num}.png\"\n",
    "    predict_draw_location(model, field_path, draw_path)\n",
    "    draw_labels_on_image(field_path,labels_file=\"truncated_labels.csv\")\n",
    "\n",
    "run(88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
