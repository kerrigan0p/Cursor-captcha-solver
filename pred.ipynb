{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.082991 84.090775 50.159946 87.09453\n",
      "(73, 184, 115, 187)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class DrawLocatorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DrawLocatorNet, self).__init__()\n",
    "        # Field branch: for the field image (210x210)\n",
    "        # We use three convolutional blocks.\n",
    "        self.field_cnn = nn.Sequential(\n",
    "            # Block 1: 210x210 -> 105x105\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # (B,32,210,210)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                           # (B,32,105,105)\n",
    "            \n",
    "            # Block 2: 105x105 -> 52x52 (approx.)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # (B,64,105,105)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                           # (B,64,52,52)\n",
    "            \n",
    "            # Block 3: 52x52 -> 26x26\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),# (B,128,52,52)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                            # (B,128,26,26)\n",
    "        )\n",
    "        \n",
    "        # Draw branch: for the draw image (50x50)\n",
    "        # We use three convolutional blocks and an adaptive average pooling to get a feature vector.\n",
    "        self.draw_cnn = nn.Sequential(\n",
    "            # Block 1: 50x50 -> 25x25\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),   # (B,32,50,50)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                              # (B,32,25,25)\n",
    "            \n",
    "            # Block 2: 25x25 -> 12x12 (approx.)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),   # (B,64,25,25)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                              # (B,64,12,12)\n",
    "            \n",
    "            # Block 3: 12x12 remains 12x12 but increases channels\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # (B,128,12,12)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))                    # (B,128,1,1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, field_img, draw_img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            field_img: Tensor of shape (B,3,210,210)\n",
    "            draw_img: Tensor of shape (B,3,50,50)\n",
    "        Returns:\n",
    "            preds: Tensor of shape (B,2) with the predicted (x, y) coordinates normalized in [0,1].\n",
    "        \"\"\"\n",
    "        # Extract features from the field image; shape: (B,128,26,26)\n",
    "        field_feat = self.field_cnn(field_img)\n",
    "        \n",
    "        # Extract feature vector from the draw image; shape: (B,128,1,1)\n",
    "        draw_feat = self.draw_cnn(draw_img)\n",
    "        draw_feat = draw_feat.view(draw_feat.size(0), -1)  # (B,128)\n",
    "        \n",
    "        # Compute the correlation map:\n",
    "        # For each spatial location in the field feature map, compute the dot product with the draw feature vector.\n",
    "        # Result: (B,26,26)\n",
    "        correlation = (field_feat * draw_feat.view(draw_feat.size(0), draw_feat.size(1), 1, 1)).sum(dim=1)\n",
    "        \n",
    "        # Get spatial dimensions (H, W) of the correlation map\n",
    "        B, H, W = correlation.size()\n",
    "        \n",
    "        # Create a coordinate grid corresponding to the correlation map.\n",
    "        device = correlation.device\n",
    "        grid_y, grid_x = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing='ij')\n",
    "        grid_x = grid_x.float()\n",
    "        grid_y = grid_y.float()\n",
    "        \n",
    "        # Flatten the correlation map and compute softmax to obtain a probability distribution over spatial locations.\n",
    "        correlation_flat = correlation.view(B, -1)  # shape: (B, H*W)\n",
    "        prob = F.softmax(correlation_flat, dim=1).view(B, H, W)  # shape: (B, H, W)\n",
    "        \n",
    "        # Compute the expected coordinates (soft-argmax) in the feature map grid.\n",
    "        pred_x = (prob * grid_x).view(B, -1).sum(dim=1)\n",
    "        pred_y = (prob * grid_y).view(B, -1).sum(dim=1)\n",
    "        \n",
    "        # The predicted coordinates are in the feature map scale (26x26).\n",
    "        # To normalize them to [0,1], we simply divide by the width/height of the feature map.\n",
    "        pred_x = pred_x / W\n",
    "        pred_y = pred_y / H\n",
    "        \n",
    "        preds = torch.stack([pred_x, pred_y], dim=1)  # shape: (B,2)\n",
    "        return preds\n",
    "\n",
    "\n",
    "def get_information_zone(img):\n",
    "    top_pixels,bot_pixels,left_pixels,right_pixels = 310,100,65,275 # Field coordonates\n",
    "    field_img = img[bot_pixels:top_pixels, left_pixels:right_pixels]\n",
    "\n",
    "    top_pixels,bot_pixels,left_pixels,right_pixels = 55,5,185,235 # Draw 1 coordonates\n",
    "    draw1_img = img[bot_pixels:top_pixels, left_pixels:right_pixels] \n",
    "\n",
    "    top_pixels, bot_pixels, left_pixels, right_pixels = 50,10, 265, 305 #Draw 2 coordonates\n",
    "    draw2_img = img[bot_pixels:top_pixels, left_pixels:right_pixels]\n",
    "\n",
    "    return field_img, draw1_img, draw2_img\n",
    "\n",
    "\n",
    "def normalize_img(img, mean = (0.83977205, 0.8524061, 0.55467314), std = (0.2027646, 0.18541439, 0.18301369)): #mean and std comes from a study in the dataset used for training model\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    img = img.astype(np.float32) \n",
    "\n",
    "    # Normalize: (pixel - mean) / std\n",
    "    img = (img - mean) / std\n",
    "\n",
    "    # Convert back to 0-255 range \n",
    "    img = ((img - img.min()) / (img.max() - img.min()) * 255).astype(np.uint8)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def make_predictions(field_img, draw1_img, draw2_img, model_path, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),   # Convert to tensor (C, H, W) and scale to [0,1]\n",
    "    ])\n",
    "    field_img = transform(field_img).unsqueeze(0).to(device)  # Shape: (1, 3, 210, 210)\n",
    "    draw1_img = transform(draw1_img).unsqueeze(0).to(device)    # Shape: (1, 3, 50, 50)\n",
    "    draw2_img = transform(draw2_img).unsqueeze(0).to(device)    # Shape: (1, 3, 50, 50)\n",
    "    model = DrawLocatorNet()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        pred1_coords = model(field_img, draw1_img).cpu().numpy()[0]  # Shape: (2,)\n",
    "        pred2_coords = model(field_img, draw2_img).cpu().numpy()[0]  # Shape: (2,)\n",
    "    # Extract and return the (x, y) coordinates\n",
    "    pred_x1, pred_y1 = pred1_coords*210\n",
    "    pred_x2, pred_y2 = pred2_coords*210\n",
    "    print(pred_x1, pred_y1, pred_x2, pred_y2)\n",
    "    #Give useful coordonates\n",
    "    bot_pixels, left_pixels = 100, 65\n",
    "    x1, x2 = int(pred_x1) + left_pixels, int(pred_x2) + left_pixels\n",
    "    y1, y2 = int(pred_y1) + bot_pixels, int(pred_y2) + bot_pixels\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "def run(img, model_path = \"Dev/training/e100_m20_g50.pth\"):\n",
    "\n",
    "    field_img, draw1_img, draw2_img = get_information_zone(img)\n",
    "\n",
    "    field_img, draw1_img, draw2_img = cv2.cvtColor(normalize_img(field_img), cv2.COLOR_RGB2BGR), cv2.cvtColor(normalize_img(draw1_img), cv2.COLOR_RGB2BGR), cv2.cvtColor(normalize_img(draw2_img), cv2.COLOR_RGB2BGR)\n",
    "    for img in [field_img, draw1_img, draw2_img]:\n",
    "        cv2.imshow(\"img\",img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "    x1, y1, x2, y2 = make_predictions(field_img, draw1_img, draw2_img, model_path)\n",
    "\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "image_path = 'Dev\\extracted_captchas\\captchas_saved\\captcha_87.png'\n",
    "image = cv2.imread(image_path)\n",
    "print(run(img=image))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
