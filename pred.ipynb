{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predictions captcha 2: (184, 206, 73, 132)\n",
      "True labels captcha 2 : (182,207,75,131) \n",
      "Model predictions captcha 86: (195, 235, 218, 156)\n",
      "True labels captcha 86 : (192,238,217,163) \n",
      "Model predictions captcha 97: (105, 269, 200, 201)\n",
      "True labels captcha 97 : (111,271,204,193) \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class DrawLocatorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DrawLocatorNet, self).__init__()\n",
    "\n",
    "        # CNN branch for the field image (input size: 210x210)\n",
    "        self.field_cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # Convolutional layer (B,32,210,210)\n",
    "            nn.BatchNorm2d(32),  # Batch normalization\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.MaxPool2d(2),  # Downsample to (B,32,105,105)\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # (B,64,105,105)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # (B,64,52,52)\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # (B,128,52,52)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)  # (B,128,26,26)\n",
    "        )\n",
    "\n",
    "        # CNN branch for the draw image (input size: 50x50)\n",
    "        self.draw_cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # (B,32,50,50)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # (B,32,25,25)\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # (B,64,25,25)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # (B,64,12,12)\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # (B,128,12,12)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # Compress to (B,128,1,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, field_img, draw_img):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            field_img: Tensor (B,3,210,210) - Field Image\n",
    "            draw_img: Tensor (B,3,50,50) - Draw Image\n",
    "            \n",
    "        Returns:\n",
    "            preds: Tensor (B,2) - Predicted (x, y) coordinates normalized in [0,1].\n",
    "        \"\"\"\n",
    "        # Extract field image features (B,128,26,26)\n",
    "        field_feat = self.field_cnn(field_img)\n",
    "\n",
    "        # Extract draw image features (B,128,1,1) and flatten to (B,128)\n",
    "        draw_feat = self.draw_cnn(draw_img)\n",
    "        draw_feat = draw_feat.view(draw_feat.size(0), -1)\n",
    "\n",
    "        # Compute similarity map (B,26,26) between draw image and field feature map\n",
    "        correlation = (field_feat * draw_feat.view(draw_feat.size(0), draw_feat.size(1), 1, 1)).sum(dim=1)\n",
    "\n",
    "        # Get correlation map dimensions\n",
    "        B, H, W = correlation.size()\n",
    "\n",
    "        # Generate grid for spatial coordinates\n",
    "        device = correlation.device\n",
    "        grid_y, grid_x = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing='ij')\n",
    "        grid_x = grid_x.float()\n",
    "        grid_y = grid_y.float()\n",
    "\n",
    "        # Compute probability distribution using softmax\n",
    "        correlation_flat = correlation.view(B, -1)  # Flatten (B, H*W)\n",
    "        prob = F.softmax(correlation_flat, dim=1).view(B, H, W)\n",
    "\n",
    "        # Compute expected (x, y) coordinates using soft-argmax\n",
    "        pred_x = (prob * grid_x).view(B, -1).sum(dim=1) / W\n",
    "        pred_y = (prob * grid_y).view(B, -1).sum(dim=1) / H\n",
    "\n",
    "        preds = torch.stack([pred_x, pred_y], dim=1)  # (B,2)\n",
    "        return preds\n",
    "\n",
    "\n",
    "def get_information_zone(img):\n",
    "    \"\"\"\n",
    "    Extract specific regions from the input image.\n",
    "\n",
    "    Args:\n",
    "        img: np.array (original image)\n",
    "\n",
    "    Returns:\n",
    "        field_img: Cropped field image (main field)\n",
    "        draw1_img: First cropped draw image\n",
    "        draw2_img: Second cropped draw image\n",
    "    \"\"\"\n",
    "    # Coordinates for cropping different parts of the image\n",
    "    field_img = img[100:310, 65:275]  # Field coordinates\n",
    "    draw1_img = img[5:55, 185:235]  # Draw 1 coordinates\n",
    "    draw2_img = img[5:55, 260:310]  # Draw 2 coordinates\n",
    "\n",
    "    return field_img, draw1_img, draw2_img\n",
    "\n",
    "\n",
    "def cv2_to_pil(cv2_img):\n",
    "    \"\"\"\n",
    "    Convert a cv2 image (BGR format) to a PIL Image in RGB format.\n",
    "    \"\"\"\n",
    "    rgb_img = cv2.cvtColor(cv2_img, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(rgb_img)\n",
    "    return pil_img\n",
    "\n",
    "\n",
    "def make_predictions(field_img, draw1_img, draw2_img, model_path, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Perform inference using the trained model.\n",
    "\n",
    "    Args:\n",
    "        field_img: PIL Image (Field)\n",
    "        draw1_img: PIL Image (Draw 1)\n",
    "        draw2_img: PIL Image (Draw 2)\n",
    "        model_path: str - Path to the trained model\n",
    "        device: str - \"cuda\" or \"cpu\"\n",
    "\n",
    "    Returns:\n",
    "        (x1, y1, x2, y2): Tuple with predicted pixel coordinates.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()  # Convert image to tensor\n",
    "    ])\n",
    "    \n",
    "    # Apply transformations and add batch dimension\n",
    "    field_img = transform(field_img).unsqueeze(0).to(device)  # (1, 3, 210, 210)\n",
    "    draw1_img = transform(draw1_img).unsqueeze(0).to(device)  # (1, 3, 50, 50)\n",
    "    draw2_img = transform(draw2_img).unsqueeze(0).to(device)  # (1, 3, 50, 50)\n",
    "\n",
    "    # Load model\n",
    "    model = DrawLocatorNet()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval().to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        pred1_coords = model(field_img, draw1_img).cpu().numpy()[0]  # (2,)\n",
    "        pred2_coords = model(field_img, draw2_img).cpu().numpy()[0]  # (2,)\n",
    "\n",
    "    # Scale predictions to image dimensions (from normalized [0,1] to [0,210])\n",
    "    pred_x1, pred_y1 = pred1_coords * 210\n",
    "    pred_x2, pred_y2 = pred2_coords * 210\n",
    "\n",
    "    # Adjust predictions relative to the original image (accounting for cropping)\n",
    "    x1, x2 = int(pred_x1) + 65, int(pred_x2) + 65  # Offset by left_pixels (65)\n",
    "    y1, y2 = int(pred_y1) + 100, int(pred_y2) + 100  # Offset by bot_pixels (100)\n",
    "\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "def run(img, model_path=\"model.pth\"):\n",
    "    \"\"\"\n",
    "    Full pipeline to process an image and get predictions.\n",
    "\n",
    "    Args:\n",
    "        img: np.array - Original image loaded using cv2\n",
    "        model_path: str - Path to the trained model\n",
    "\n",
    "    Returns:\n",
    "        (x1, y1, x2, y2): Predicted coordinates\n",
    "    \"\"\"\n",
    "    # Extract relevant regions\n",
    "    field_img, draw1_img, draw2_img = get_information_zone(img)\n",
    "\n",
    "    # Convert OpenCV images to PIL format\n",
    "    field_img, draw1_img, draw2_img = cv2_to_pil(field_img), cv2_to_pil(draw1_img), cv2_to_pil(draw2_img)\n",
    "\n",
    "    # Perform inference\n",
    "    return make_predictions(field_img, draw1_img, draw2_img, model_path)\n",
    "\n",
    "\n",
    "# Tests\n",
    "# Captcha 2\n",
    "image_path = 'Dev\\\\extracted_captchas\\\\captchas_saved\\\\captcha_2.png'\n",
    "image = cv2.imread(image_path)\n",
    "print(f'Model predictions captcha 2: {run(img=image)}')\n",
    "print(f\"True labels captcha 2 : (182,207,75,131) \")\n",
    "\n",
    "# Captcha 86\n",
    "image_path = 'Dev\\\\extracted_captchas\\\\captchas_saved\\\\captcha_86.png'\n",
    "image = cv2.imread(image_path)\n",
    "print(f'Model predictions captcha 86: {run(img=image)}')\n",
    "print(f\"True labels captcha 86 : (192,238,217,163) \")\n",
    "\n",
    "# Captcha 97\n",
    "image_path = 'Dev\\\\extracted_captchas\\\\captchas_saved\\\\captcha_97.png'\n",
    "image = cv2.imread(image_path)\n",
    "print(f'Model predictions captcha 97: {run(img=image)}')\n",
    "print(f\"True labels captcha 97 : (111,271,204,193) \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
