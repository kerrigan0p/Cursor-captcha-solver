{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "id": "TEBgRGBSAaeW",
        "outputId": "19bcdec8-4fbf-4374-cd69-a04f06f875d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ground truth coordinates: x1=105, y1=175, x2=180, y2=207\n",
            "Ground truth coordinates: x1=40, y1=75, x2=115, y2=107\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def draw_labels_on_image(image_path, labels_file):\n",
        "    \"\"\"\n",
        "    Loads an image from image_path, retrieves ground truth (x1, y1, x2, y2) from labels.txt,\n",
        "    and displays the image with red circles at those coordinates.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image.\n",
        "        labels_file (str): Path to the labels file (CSV or TXT with x1, y1, x2, y2).\n",
        "    \"\"\"\n",
        "    # Load the image using OpenCV\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Unable to load image: {image_path}\")\n",
        "        return\n",
        "\n",
        "    # Resize the image to (width=340, height=410) to match the model's expected size\n",
        "    image_resized = image#cv2.resize(image, (340, 410))\n",
        "\n",
        "    # Read labels from the file\n",
        "    labels_df = pd.read_csv(labels_file)  # Ensure labels.txt is formatted correctly\n",
        "    image_name = image_path.split('/')[-1]  # Extract filename from path\n",
        "\n",
        "    # Find the row corresponding to the image name (assuming there is an 'id' or filename column)\n",
        "    if 'img_name' in labels_df.columns:\n",
        "        row = labels_df[labels_df['img_name'] == image_name]\n",
        "    else:\n",
        "        row = labels_df.iloc[0]  # If there's no ID column, just use the first row (for testing)\n",
        "\n",
        "    if row.empty:\n",
        "        print(f\"No labels found for {image_name}\")\n",
        "        return\n",
        "\n",
        "    # Extract ground truth coordinates\n",
        "    x1, y1, x2, y2 = row[['x1', 'y1', 'x2', 'y2']].to_numpy().flatten()\n",
        "    x1 = int(x1)\n",
        "    x2 = int(x2)\n",
        "    y1 = int(y1)\n",
        "    y2 = int(y2)\n",
        "    print(f\"Ground truth coordinates: x1={x1}, y1={y1}, x2={x2}, y2={y2}\")\n",
        "\n",
        "    # Draw red circles at the ground truth coordinates\n",
        "    image_drawn = image_resized.copy()\n",
        "    cv2.circle(image_drawn, (x1, y1), radius=5, color=(0, 255, 0), thickness=-1)  # Green circle\n",
        "    cv2.circle(image_drawn, (x2, y2), radius=5, color=(0, 0, 255), thickness=-1)  # Red circle\n",
        "\n",
        "    # Display the image with the drawn points\n",
        "    cv2.imshow(\"img\",image_drawn)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "extract_dir = 'extracted_captchas'\n",
        "processed_images_dir = \"processed_captchas\"\n",
        "image_path = f\"{extract_dir}/captchas_saved/captcha_8.png\"\n",
        "labels_file = \"labels.txt\"\n",
        "draw_labels_on_image(image_path,labels_file)\n",
        "image_path = f\"truncated_captchas/captcha_8.png\"\n",
        "labels_file = \"truncated_labels.csv\"\n",
        "draw_labels_on_image(image_path,labels_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "Pamd0yPYBJhm",
        "outputId": "3901a604-edf0-421b-f359-220a765b6f85"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def show_pixels(image_path,nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite):\n",
        "    \"\"\"\n",
        "    Displays the top 20 rows of an image.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file.\n",
        "    \"\"\"\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Check if the image was loaded correctly\n",
        "    if image is None:\n",
        "        print(f\"Error: Unable to load image at {image_path}\")\n",
        "        return\n",
        "\n",
        "    truncated_image = image[nbPixelsBas:nbPixelsHaut, nbPixelsGauche:nbPixelsDroite]\n",
        "\n",
        "    # Display the cropped section\n",
        "    cv2.imshow(\"img\",truncated_image)  # Use cv2_imshow in Google Colab, replace with cv2.imshow() for local use\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "extract_dir = 'extracted_captchas'\n",
        "processed_images_dir = \"processed_captchas\"\n",
        "image_path = f\"{extract_dir}/captchas_saved/captcha_315.png\"\n",
        "labels_file = \"labels.txt\"\n",
        "\n",
        "nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite = 310,100,65,275 # Images tronquées\n",
        "# nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite = 55,5,185,235 # Premier perso\n",
        "# nbPixelsHaut, nbPixelsBas, nbPixelsGauche, nbPixelsDroite = 50,10, 265, 305 #Second perso\n",
        "show_pixels(image_path,nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkXTRcDNEJnf",
        "outputId": "2d998411-6424-4b43-b5d6-58ca188128b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['img_name', 'x1', 'y1', 'x2', 'y2'], dtype='object')\n",
            "            img_name          x1          y1          x2          y2\n",
            "0      captcha_1.png   46.816017  146.209957   54.581169   18.639610\n",
            "1     captcha_10.png   54.581169  143.991342  155.528139   67.449134\n",
            "2    captcha_100.png  103.390693  192.800866   22.411255  191.691558\n",
            "3    captcha_101.png  142.216450  125.133117   60.127706  165.068182\n",
            "4    captcha_102.png   95.625541  130.679654  155.528139  170.614719\n",
            "..               ...         ...         ...         ...         ...\n",
            "311   captcha_95.png   83.423160   35.279221  122.248918  182.817100\n",
            "312   captcha_96.png   49.034632   40.825758   44.597403  172.833333\n",
            "313   captcha_97.png   45.706710  170.614719  138.888528   92.963203\n",
            "314   captcha_98.png   45.706710  105.165584  176.604978   49.700216\n",
            "315   captcha_99.png   42.378788  187.254329   23.520563  117.367965\n",
            "\n",
            "[316 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def adjust_x_coordinates(label, nbPixelsGauche):\n",
        "    return label - nbPixelsGauche\n",
        "\n",
        "def adjust_y_coordinates(label, nbPixelsBas):\n",
        "    return label - nbPixelsBas\n",
        "\n",
        "# Load the labels file\n",
        "df = pd.read_csv(\"labels.txt\")\n",
        "print(df.columns)\n",
        "# Ensure the coordinate columns are numeric\n",
        "df[['x1', 'y1', 'x2', 'y2']] = df[['x1', 'y1', 'x2', 'y2']].apply(pd.to_numeric)\n",
        "\n",
        "nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite = 310,100,65,275 # Images tronquées\n",
        "# Adjust coordinates\n",
        "df['x1'] = df['x1'].apply(lambda x: adjust_x_coordinates(x, nbPixelsGauche))\n",
        "df['x2'] = df['x2'].apply(lambda x: adjust_x_coordinates(x, nbPixelsGauche))\n",
        "df['y1'] = df['y1'].apply(lambda y: adjust_y_coordinates(y, nbPixelsBas))\n",
        "df['y2'] = df['y2'].apply(lambda y: adjust_y_coordinates(y, nbPixelsBas))\n",
        "\n",
        "# Save the adjusted labels\n",
        "df.to_csv('truncated_labels.csv', index=False)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZZ0f7xyI9YM",
        "outputId": "d30f2318-a07a-4fd4-b7ad-b683490af353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved cropped image: truncated_captchas\\captcha_1.png\n",
            "Saved cropped image: truncated_captchas\\captcha_10.png\n",
            "Saved cropped image: truncated_captchas\\captcha_100.png\n",
            "Saved cropped image: truncated_captchas\\captcha_101.png\n",
            "Saved cropped image: truncated_captchas\\captcha_102.png\n",
            "Saved cropped image: truncated_captchas\\captcha_103.png\n",
            "Saved cropped image: truncated_captchas\\captcha_104.png\n",
            "Saved cropped image: truncated_captchas\\captcha_105.png\n",
            "Saved cropped image: truncated_captchas\\captcha_106.png\n",
            "Saved cropped image: truncated_captchas\\captcha_107.png\n",
            "Saved cropped image: truncated_captchas\\captcha_108.png\n",
            "Saved cropped image: truncated_captchas\\captcha_109.png\n",
            "Saved cropped image: truncated_captchas\\captcha_11.png\n",
            "Saved cropped image: truncated_captchas\\captcha_110.png\n",
            "Saved cropped image: truncated_captchas\\captcha_111.png\n",
            "Saved cropped image: truncated_captchas\\captcha_112.png\n",
            "Saved cropped image: truncated_captchas\\captcha_113.png\n",
            "Saved cropped image: truncated_captchas\\captcha_114.png\n",
            "Saved cropped image: truncated_captchas\\captcha_115.png\n",
            "Saved cropped image: truncated_captchas\\captcha_116.png\n",
            "Saved cropped image: truncated_captchas\\captcha_117.png\n",
            "Saved cropped image: truncated_captchas\\captcha_118.png\n",
            "Saved cropped image: truncated_captchas\\captcha_119.png\n",
            "Saved cropped image: truncated_captchas\\captcha_12.png\n",
            "Saved cropped image: truncated_captchas\\captcha_120.png\n",
            "Saved cropped image: truncated_captchas\\captcha_121.png\n",
            "Saved cropped image: truncated_captchas\\captcha_122.png\n",
            "Saved cropped image: truncated_captchas\\captcha_123.png\n",
            "Saved cropped image: truncated_captchas\\captcha_124.png\n",
            "Saved cropped image: truncated_captchas\\captcha_125.png\n",
            "Saved cropped image: truncated_captchas\\captcha_126.png\n",
            "Saved cropped image: truncated_captchas\\captcha_127.png\n",
            "Saved cropped image: truncated_captchas\\captcha_128.png\n",
            "Saved cropped image: truncated_captchas\\captcha_129.png\n",
            "Saved cropped image: truncated_captchas\\captcha_13.png\n",
            "Saved cropped image: truncated_captchas\\captcha_130.png\n",
            "Saved cropped image: truncated_captchas\\captcha_131.png\n",
            "Saved cropped image: truncated_captchas\\captcha_132.png\n",
            "Saved cropped image: truncated_captchas\\captcha_133.png\n",
            "Saved cropped image: truncated_captchas\\captcha_134.png\n",
            "Saved cropped image: truncated_captchas\\captcha_135.png\n",
            "Saved cropped image: truncated_captchas\\captcha_136.png\n",
            "Saved cropped image: truncated_captchas\\captcha_137.png\n",
            "Saved cropped image: truncated_captchas\\captcha_138.png\n",
            "Saved cropped image: truncated_captchas\\captcha_139.png\n",
            "Saved cropped image: truncated_captchas\\captcha_14.png\n",
            "Saved cropped image: truncated_captchas\\captcha_140.png\n",
            "Saved cropped image: truncated_captchas\\captcha_141.png\n",
            "Saved cropped image: truncated_captchas\\captcha_142.png\n",
            "Saved cropped image: truncated_captchas\\captcha_143.png\n",
            "Saved cropped image: truncated_captchas\\captcha_144.png\n",
            "Saved cropped image: truncated_captchas\\captcha_145.png\n",
            "Saved cropped image: truncated_captchas\\captcha_146.png\n",
            "Saved cropped image: truncated_captchas\\captcha_147.png\n",
            "Saved cropped image: truncated_captchas\\captcha_148.png\n",
            "Saved cropped image: truncated_captchas\\captcha_149.png\n",
            "Saved cropped image: truncated_captchas\\captcha_15.png\n",
            "Saved cropped image: truncated_captchas\\captcha_150.png\n",
            "Saved cropped image: truncated_captchas\\captcha_151.png\n",
            "Saved cropped image: truncated_captchas\\captcha_152.png\n",
            "Saved cropped image: truncated_captchas\\captcha_153.png\n",
            "Saved cropped image: truncated_captchas\\captcha_154.png\n",
            "Saved cropped image: truncated_captchas\\captcha_155.png\n",
            "Saved cropped image: truncated_captchas\\captcha_156.png\n",
            "Saved cropped image: truncated_captchas\\captcha_157.png\n",
            "Saved cropped image: truncated_captchas\\captcha_158.png\n",
            "Saved cropped image: truncated_captchas\\captcha_159.png\n",
            "Saved cropped image: truncated_captchas\\captcha_16.png\n",
            "Saved cropped image: truncated_captchas\\captcha_160.png\n",
            "Saved cropped image: truncated_captchas\\captcha_161.png\n",
            "Saved cropped image: truncated_captchas\\captcha_162.png\n",
            "Saved cropped image: truncated_captchas\\captcha_163.png\n",
            "Saved cropped image: truncated_captchas\\captcha_164.png\n",
            "Saved cropped image: truncated_captchas\\captcha_165.png\n",
            "Saved cropped image: truncated_captchas\\captcha_166.png\n",
            "Saved cropped image: truncated_captchas\\captcha_167.png\n",
            "Saved cropped image: truncated_captchas\\captcha_168.png\n",
            "Saved cropped image: truncated_captchas\\captcha_169.png\n",
            "Saved cropped image: truncated_captchas\\captcha_17.png\n",
            "Saved cropped image: truncated_captchas\\captcha_170.png\n",
            "Saved cropped image: truncated_captchas\\captcha_171.png\n",
            "Saved cropped image: truncated_captchas\\captcha_172.png\n",
            "Saved cropped image: truncated_captchas\\captcha_173.png\n",
            "Saved cropped image: truncated_captchas\\captcha_174.png\n",
            "Saved cropped image: truncated_captchas\\captcha_175.png\n",
            "Saved cropped image: truncated_captchas\\captcha_176.png\n",
            "Saved cropped image: truncated_captchas\\captcha_177.png\n",
            "Saved cropped image: truncated_captchas\\captcha_178.png\n",
            "Saved cropped image: truncated_captchas\\captcha_179.png\n",
            "Saved cropped image: truncated_captchas\\captcha_18.png\n",
            "Saved cropped image: truncated_captchas\\captcha_180.png\n",
            "Saved cropped image: truncated_captchas\\captcha_181.png\n",
            "Saved cropped image: truncated_captchas\\captcha_182.png\n",
            "Saved cropped image: truncated_captchas\\captcha_183.png\n",
            "Saved cropped image: truncated_captchas\\captcha_184.png\n",
            "Saved cropped image: truncated_captchas\\captcha_185.png\n",
            "Saved cropped image: truncated_captchas\\captcha_186.png\n",
            "Saved cropped image: truncated_captchas\\captcha_187.png\n",
            "Saved cropped image: truncated_captchas\\captcha_188.png\n",
            "Saved cropped image: truncated_captchas\\captcha_189.png\n",
            "Saved cropped image: truncated_captchas\\captcha_19.png\n",
            "Saved cropped image: truncated_captchas\\captcha_190.png\n",
            "Saved cropped image: truncated_captchas\\captcha_191.png\n",
            "Saved cropped image: truncated_captchas\\captcha_192.png\n",
            "Saved cropped image: truncated_captchas\\captcha_193.png\n",
            "Saved cropped image: truncated_captchas\\captcha_194.png\n",
            "Saved cropped image: truncated_captchas\\captcha_195.png\n",
            "Saved cropped image: truncated_captchas\\captcha_196.png\n",
            "Saved cropped image: truncated_captchas\\captcha_197.png\n",
            "Saved cropped image: truncated_captchas\\captcha_198.png\n",
            "Saved cropped image: truncated_captchas\\captcha_199.png\n",
            "Saved cropped image: truncated_captchas\\captcha_2.png\n",
            "Saved cropped image: truncated_captchas\\captcha_20.png\n",
            "Saved cropped image: truncated_captchas\\captcha_200.png\n",
            "Saved cropped image: truncated_captchas\\captcha_201.png\n",
            "Saved cropped image: truncated_captchas\\captcha_202.png\n",
            "Saved cropped image: truncated_captchas\\captcha_203.png\n",
            "Saved cropped image: truncated_captchas\\captcha_204.png\n",
            "Saved cropped image: truncated_captchas\\captcha_205.png\n",
            "Saved cropped image: truncated_captchas\\captcha_206.png\n",
            "Saved cropped image: truncated_captchas\\captcha_207.png\n",
            "Saved cropped image: truncated_captchas\\captcha_208.png\n",
            "Saved cropped image: truncated_captchas\\captcha_209.png\n",
            "Saved cropped image: truncated_captchas\\captcha_21.png\n",
            "Saved cropped image: truncated_captchas\\captcha_210.png\n",
            "Saved cropped image: truncated_captchas\\captcha_211.png\n",
            "Saved cropped image: truncated_captchas\\captcha_212.png\n",
            "Saved cropped image: truncated_captchas\\captcha_213.png\n",
            "Saved cropped image: truncated_captchas\\captcha_214.png\n",
            "Saved cropped image: truncated_captchas\\captcha_215.png\n",
            "Saved cropped image: truncated_captchas\\captcha_216.png\n",
            "Saved cropped image: truncated_captchas\\captcha_217.png\n",
            "Saved cropped image: truncated_captchas\\captcha_218.png\n",
            "Saved cropped image: truncated_captchas\\captcha_219.png\n",
            "Saved cropped image: truncated_captchas\\captcha_22.png\n",
            "Saved cropped image: truncated_captchas\\captcha_220.png\n",
            "Saved cropped image: truncated_captchas\\captcha_221.png\n",
            "Saved cropped image: truncated_captchas\\captcha_222.png\n",
            "Saved cropped image: truncated_captchas\\captcha_223.png\n",
            "Saved cropped image: truncated_captchas\\captcha_224.png\n",
            "Saved cropped image: truncated_captchas\\captcha_225.png\n",
            "Saved cropped image: truncated_captchas\\captcha_226.png\n",
            "Saved cropped image: truncated_captchas\\captcha_227.png\n",
            "Saved cropped image: truncated_captchas\\captcha_228.png\n",
            "Saved cropped image: truncated_captchas\\captcha_229.png\n",
            "Saved cropped image: truncated_captchas\\captcha_23.png\n",
            "Saved cropped image: truncated_captchas\\captcha_230.png\n",
            "Saved cropped image: truncated_captchas\\captcha_231.png\n",
            "Saved cropped image: truncated_captchas\\captcha_232.png\n",
            "Saved cropped image: truncated_captchas\\captcha_233.png\n",
            "Saved cropped image: truncated_captchas\\captcha_234.png\n",
            "Saved cropped image: truncated_captchas\\captcha_235.png\n",
            "Saved cropped image: truncated_captchas\\captcha_236.png\n",
            "Saved cropped image: truncated_captchas\\captcha_237.png\n",
            "Saved cropped image: truncated_captchas\\captcha_238.png\n",
            "Saved cropped image: truncated_captchas\\captcha_239.png\n",
            "Saved cropped image: truncated_captchas\\captcha_24.png\n",
            "Saved cropped image: truncated_captchas\\captcha_240.png\n",
            "Saved cropped image: truncated_captchas\\captcha_241.png\n",
            "Saved cropped image: truncated_captchas\\captcha_242.png\n",
            "Saved cropped image: truncated_captchas\\captcha_243.png\n",
            "Saved cropped image: truncated_captchas\\captcha_244.png\n",
            "Saved cropped image: truncated_captchas\\captcha_245.png\n",
            "Saved cropped image: truncated_captchas\\captcha_246.png\n",
            "Saved cropped image: truncated_captchas\\captcha_247.png\n",
            "Saved cropped image: truncated_captchas\\captcha_248.png\n",
            "Saved cropped image: truncated_captchas\\captcha_249.png\n",
            "Saved cropped image: truncated_captchas\\captcha_25.png\n",
            "Saved cropped image: truncated_captchas\\captcha_250.png\n",
            "Saved cropped image: truncated_captchas\\captcha_251.png\n",
            "Saved cropped image: truncated_captchas\\captcha_252.png\n",
            "Saved cropped image: truncated_captchas\\captcha_253.png\n",
            "Saved cropped image: truncated_captchas\\captcha_254.png\n",
            "Saved cropped image: truncated_captchas\\captcha_255.png\n",
            "Saved cropped image: truncated_captchas\\captcha_256.png\n",
            "Saved cropped image: truncated_captchas\\captcha_257.png\n",
            "Saved cropped image: truncated_captchas\\captcha_258.png\n",
            "Saved cropped image: truncated_captchas\\captcha_259.png\n",
            "Saved cropped image: truncated_captchas\\captcha_26.png\n",
            "Saved cropped image: truncated_captchas\\captcha_260.png\n",
            "Saved cropped image: truncated_captchas\\captcha_261.png\n",
            "Saved cropped image: truncated_captchas\\captcha_262.png\n",
            "Saved cropped image: truncated_captchas\\captcha_263.png\n",
            "Saved cropped image: truncated_captchas\\captcha_264.png\n",
            "Saved cropped image: truncated_captchas\\captcha_265.png\n",
            "Saved cropped image: truncated_captchas\\captcha_266.png\n",
            "Saved cropped image: truncated_captchas\\captcha_267.png\n",
            "Saved cropped image: truncated_captchas\\captcha_268.png\n",
            "Saved cropped image: truncated_captchas\\captcha_269.png\n",
            "Saved cropped image: truncated_captchas\\captcha_27.png\n",
            "Saved cropped image: truncated_captchas\\captcha_270.png\n",
            "Saved cropped image: truncated_captchas\\captcha_271.png\n",
            "Saved cropped image: truncated_captchas\\captcha_272.png\n",
            "Saved cropped image: truncated_captchas\\captcha_273.png\n",
            "Saved cropped image: truncated_captchas\\captcha_274.png\n",
            "Saved cropped image: truncated_captchas\\captcha_275.png\n",
            "Saved cropped image: truncated_captchas\\captcha_276.png\n",
            "Saved cropped image: truncated_captchas\\captcha_277.png\n",
            "Saved cropped image: truncated_captchas\\captcha_278.png\n",
            "Saved cropped image: truncated_captchas\\captcha_279.png\n",
            "Saved cropped image: truncated_captchas\\captcha_28.png\n",
            "Saved cropped image: truncated_captchas\\captcha_280.png\n",
            "Saved cropped image: truncated_captchas\\captcha_281.png\n",
            "Saved cropped image: truncated_captchas\\captcha_282.png\n",
            "Saved cropped image: truncated_captchas\\captcha_283.png\n",
            "Saved cropped image: truncated_captchas\\captcha_284.png\n",
            "Saved cropped image: truncated_captchas\\captcha_285.png\n",
            "Saved cropped image: truncated_captchas\\captcha_286.png\n",
            "Saved cropped image: truncated_captchas\\captcha_287.png\n",
            "Saved cropped image: truncated_captchas\\captcha_288.png\n",
            "Saved cropped image: truncated_captchas\\captcha_289.png\n",
            "Saved cropped image: truncated_captchas\\captcha_29.png\n",
            "Saved cropped image: truncated_captchas\\captcha_290.png\n",
            "Saved cropped image: truncated_captchas\\captcha_291.png\n",
            "Saved cropped image: truncated_captchas\\captcha_292.png\n",
            "Saved cropped image: truncated_captchas\\captcha_293.png\n",
            "Saved cropped image: truncated_captchas\\captcha_294.png\n",
            "Saved cropped image: truncated_captchas\\captcha_295.png\n",
            "Saved cropped image: truncated_captchas\\captcha_296.png\n",
            "Saved cropped image: truncated_captchas\\captcha_297.png\n",
            "Saved cropped image: truncated_captchas\\captcha_298.png\n",
            "Saved cropped image: truncated_captchas\\captcha_299.png\n",
            "Saved cropped image: truncated_captchas\\captcha_3.png\n",
            "Saved cropped image: truncated_captchas\\captcha_30.png\n",
            "Saved cropped image: truncated_captchas\\captcha_300.png\n",
            "Saved cropped image: truncated_captchas\\captcha_301.png\n",
            "Saved cropped image: truncated_captchas\\captcha_302.png\n",
            "Saved cropped image: truncated_captchas\\captcha_303.png\n",
            "Saved cropped image: truncated_captchas\\captcha_304.png\n",
            "Saved cropped image: truncated_captchas\\captcha_305.png\n",
            "Saved cropped image: truncated_captchas\\captcha_306.png\n",
            "Saved cropped image: truncated_captchas\\captcha_307.png\n",
            "Saved cropped image: truncated_captchas\\captcha_308.png\n",
            "Saved cropped image: truncated_captchas\\captcha_309.png\n",
            "Saved cropped image: truncated_captchas\\captcha_31.png\n",
            "Saved cropped image: truncated_captchas\\captcha_310.png\n",
            "Saved cropped image: truncated_captchas\\captcha_311.png\n",
            "Saved cropped image: truncated_captchas\\captcha_312.png\n",
            "Saved cropped image: truncated_captchas\\captcha_313.png\n",
            "Saved cropped image: truncated_captchas\\captcha_314.png\n",
            "Saved cropped image: truncated_captchas\\captcha_315.png\n",
            "Saved cropped image: truncated_captchas\\captcha_316.png\n",
            "Saved cropped image: truncated_captchas\\captcha_32.png\n",
            "Saved cropped image: truncated_captchas\\captcha_33.png\n",
            "Saved cropped image: truncated_captchas\\captcha_34.png\n",
            "Saved cropped image: truncated_captchas\\captcha_35.png\n",
            "Saved cropped image: truncated_captchas\\captcha_36.png\n",
            "Saved cropped image: truncated_captchas\\captcha_37.png\n",
            "Saved cropped image: truncated_captchas\\captcha_38.png\n",
            "Saved cropped image: truncated_captchas\\captcha_39.png\n",
            "Saved cropped image: truncated_captchas\\captcha_4.png\n",
            "Saved cropped image: truncated_captchas\\captcha_40.png\n",
            "Saved cropped image: truncated_captchas\\captcha_41.png\n",
            "Saved cropped image: truncated_captchas\\captcha_42.png\n",
            "Saved cropped image: truncated_captchas\\captcha_43.png\n",
            "Saved cropped image: truncated_captchas\\captcha_44.png\n",
            "Saved cropped image: truncated_captchas\\captcha_45.png\n",
            "Saved cropped image: truncated_captchas\\captcha_46.png\n",
            "Saved cropped image: truncated_captchas\\captcha_47.png\n",
            "Saved cropped image: truncated_captchas\\captcha_48.png\n",
            "Saved cropped image: truncated_captchas\\captcha_49.png\n",
            "Saved cropped image: truncated_captchas\\captcha_5.png\n",
            "Saved cropped image: truncated_captchas\\captcha_50.png\n",
            "Saved cropped image: truncated_captchas\\captcha_51.png\n",
            "Saved cropped image: truncated_captchas\\captcha_52.png\n",
            "Saved cropped image: truncated_captchas\\captcha_53.png\n",
            "Saved cropped image: truncated_captchas\\captcha_54.png\n",
            "Saved cropped image: truncated_captchas\\captcha_55.png\n",
            "Saved cropped image: truncated_captchas\\captcha_56.png\n",
            "Saved cropped image: truncated_captchas\\captcha_57.png\n",
            "Saved cropped image: truncated_captchas\\captcha_58.png\n",
            "Saved cropped image: truncated_captchas\\captcha_59.png\n",
            "Saved cropped image: truncated_captchas\\captcha_6.png\n",
            "Saved cropped image: truncated_captchas\\captcha_60.png\n",
            "Saved cropped image: truncated_captchas\\captcha_61.png\n",
            "Saved cropped image: truncated_captchas\\captcha_62.png\n",
            "Saved cropped image: truncated_captchas\\captcha_63.png\n",
            "Saved cropped image: truncated_captchas\\captcha_64.png\n",
            "Saved cropped image: truncated_captchas\\captcha_65.png\n",
            "Saved cropped image: truncated_captchas\\captcha_66.png\n",
            "Saved cropped image: truncated_captchas\\captcha_67.png\n",
            "Saved cropped image: truncated_captchas\\captcha_68.png\n",
            "Saved cropped image: truncated_captchas\\captcha_69.png\n",
            "Saved cropped image: truncated_captchas\\captcha_7.png\n",
            "Saved cropped image: truncated_captchas\\captcha_70.png\n",
            "Saved cropped image: truncated_captchas\\captcha_71.png\n",
            "Saved cropped image: truncated_captchas\\captcha_72.png\n",
            "Saved cropped image: truncated_captchas\\captcha_73.png\n",
            "Saved cropped image: truncated_captchas\\captcha_74.png\n",
            "Saved cropped image: truncated_captchas\\captcha_75.png\n",
            "Saved cropped image: truncated_captchas\\captcha_76.png\n",
            "Saved cropped image: truncated_captchas\\captcha_77.png\n",
            "Saved cropped image: truncated_captchas\\captcha_78.png\n",
            "Saved cropped image: truncated_captchas\\captcha_79.png\n",
            "Saved cropped image: truncated_captchas\\captcha_8.png\n",
            "Saved cropped image: truncated_captchas\\captcha_80.png\n",
            "Saved cropped image: truncated_captchas\\captcha_81.png\n",
            "Saved cropped image: truncated_captchas\\captcha_82.png\n",
            "Saved cropped image: truncated_captchas\\captcha_83.png\n",
            "Saved cropped image: truncated_captchas\\captcha_84.png\n",
            "Saved cropped image: truncated_captchas\\captcha_85.png\n",
            "Saved cropped image: truncated_captchas\\captcha_86.png\n",
            "Saved cropped image: truncated_captchas\\captcha_87.png\n",
            "Saved cropped image: truncated_captchas\\captcha_88.png\n",
            "Saved cropped image: truncated_captchas\\captcha_89.png\n",
            "Saved cropped image: truncated_captchas\\captcha_9.png\n",
            "Saved cropped image: truncated_captchas\\captcha_90.png\n",
            "Saved cropped image: truncated_captchas\\captcha_91.png\n",
            "Saved cropped image: truncated_captchas\\captcha_92.png\n",
            "Saved cropped image: truncated_captchas\\captcha_93.png\n",
            "Saved cropped image: truncated_captchas\\captcha_94.png\n",
            "Saved cropped image: truncated_captchas\\captcha_95.png\n",
            "Saved cropped image: truncated_captchas\\captcha_96.png\n",
            "Saved cropped image: truncated_captchas\\captcha_97.png\n",
            "Saved cropped image: truncated_captchas\\captcha_98.png\n",
            "Saved cropped image: truncated_captchas\\captcha_99.png\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "def crop_and_save_images(input_folder, output_folder=\"truncated_captchas\",nbPixelsHaut= 330, nbPixelsBas= 55, nbPixelsGauche= 20, nbPixelsDroite = 320):\n",
        "    \"\"\"\n",
        "    Crops images in the input_folder according to predefined pixel boundaries\n",
        "    and saves them to the output_folder with the same filenames.\n",
        "\n",
        "    Args:\n",
        "        input_folder (str): Path to the folder containing the original images.\n",
        "        output_folder (str): Path where the cropped images will be saved.\n",
        "    \"\"\"\n",
        "    # Define cropping boundaries\n",
        "    \n",
        "\n",
        "    # Create output folder if it does not exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Process each image in the input folder\n",
        "    for filename in os.listdir(input_folder):\n",
        "        input_path = os.path.join(input_folder, filename)\n",
        "        output_path = os.path.join(output_folder, filename)\n",
        "\n",
        "        # Read the image\n",
        "        image = cv2.imread(input_path)\n",
        "        if image is None:\n",
        "            print(f\"Skipping {filename} (could not load image)\")\n",
        "            continue\n",
        "\n",
        "        # Apply cropping\n",
        "        cropped_image = image[nbPixelsBas:nbPixelsHaut, nbPixelsGauche:nbPixelsDroite]\n",
        "\n",
        "        # Save the cropped image to the new folder\n",
        "        cv2.imwrite(output_path, cropped_image)\n",
        "        print(f\"Saved cropped image: {output_path}\")\n",
        "\n",
        "# nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite = 50,10,190,230 # Premier perso\n",
        "# nbPixelsHaut, nbPixelsBas, nbPixelsGauche, nbPixelsDroite = 50,10, 265, 305 #Second perso\n",
        "\n",
        "# Example Usage\n",
        "nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite = 310,100,65,275 # Images tronquées\n",
        "crop_and_save_images(\"extracted_captchas/captchas_saved\",\"truncated_captchas\",nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite) #truncated captchas\n",
        "# nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite = 55,5,185,235\n",
        "# crop_and_save_images(\"extracted_captchas/captchas_saved\",\"premier_perso\",nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite) #premier perso\n",
        "# nbPixelsHaut, nbPixelsBas, nbPixelsGauche, nbPixelsDroite = 55,5, 260, 310\n",
        "# crop_and_save_images(\"extracted_captchas/captchas_saved\",\"second_perso\",nbPixelsHaut,nbPixelsBas,nbPixelsGauche,nbPixelsDroite) #second perso\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "LuaXvUTGJGhd"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Path to the folder containing the images\n",
        "image_folder = \"truncated_captchas\"  # Change this to your actual folder path\n",
        "\n",
        "# List all image files in the folder\n",
        "image_files = [f for f in os.listdir(image_folder) if f.endswith('.png')]  # Change extension if needed\n",
        "\n",
        "# Initialize an accumulator with zeros (assuming images are the same size)\n",
        "num_images = len(image_files)\n",
        "if num_images == 0:\n",
        "    raise ValueError(\"No images found in the specified folder.\")\n",
        "\n",
        "# Load the first image to get dimensions\n",
        "first_image = cv2.imread(os.path.join(image_folder, image_files[0]), cv2.IMREAD_COLOR)\n",
        "h, w, c = first_image.shape\n",
        "mean_image = np.zeros((h, w, c), dtype=np.float32)\n",
        "\n",
        "# Compute the sum of all images\n",
        "for file in image_files:\n",
        "    img = cv2.imread(os.path.join(image_folder, file), cv2.IMREAD_COLOR)\n",
        "    if img is None:\n",
        "        print(f\"Warning: Could not read {file}\")\n",
        "        continue\n",
        "    mean_image += img.astype(np.float32)  # Convert to float to prevent overflow\n",
        "\n",
        "# Compute the mean by dividing by the number of images\n",
        "mean_image /= num_images\n",
        "\n",
        "# Convert back to uint8 format for visualization and saving\n",
        "mean_image = np.clip(mean_image, 0, 255).astype(np.uint8)\n",
        "\n",
        "# Save the mean image\n",
        "cv2.imwrite(\"mean_image.png\", mean_image)\n",
        "\n",
        "# Display the mean image (optional)\n",
        "cv2.imshow(\"Mean Image\", mean_image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned images saved in filtered_truncated_captchas\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "image_folder = \"truncated_captchas\"  # Change this to your actual folder\n",
        "output_folder = \"filtered_truncated_captchas\"  # Folder to save cleaned images\n",
        "mean_image_path = \"mean_image.png\"  # Path to saved mean image\n",
        "\n",
        "# Create output folder if it doesn't exist\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Load the mean image\n",
        "mean_image = cv2.imread(mean_image_path, cv2.IMREAD_COLOR).astype(np.float32)\n",
        "\n",
        "# List all image files in the folder\n",
        "image_files = [f for f in os.listdir(image_folder) if f.endswith('.png')]  # Change extension if needed\n",
        "\n",
        "# Process each image\n",
        "for file in image_files:\n",
        "    img_path = os.path.join(image_folder, file)\n",
        "    output_path = os.path.join(output_folder, file)\n",
        "    \n",
        "    # Load the image\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_COLOR).astype(np.float32)\n",
        "\n",
        "    # Subtract the mean image\n",
        "    cleaned_img = img - mean_image\n",
        "\n",
        "    # Normalize back to 0-255\n",
        "    cleaned_img = np.clip(cleaned_img, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # Save the cleaned image\n",
        "    cv2.imwrite(output_path, cleaned_img)\n",
        "\n",
        "print(f\"Cleaned images saved in {output_folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ground truth coordinates: x1=40, y1=75, x2=115, y2=107\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def draw_labels_on_image(image_path, labels_file):\n",
        "    \"\"\"\n",
        "    Loads an image from image_path, retrieves ground truth (x1, y1, x2, y2) from labels.txt,\n",
        "    and displays the image with red circles at those coordinates.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image.\n",
        "        labels_file (str): Path to the labels file (CSV or TXT with x1, y1, x2, y2).\n",
        "    \"\"\"\n",
        "    # Load the image using OpenCV\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Unable to load image: {image_path}\")\n",
        "        return\n",
        "\n",
        "    # Resize the image to (width=340, height=410) to match the model's expected size\n",
        "    image_resized = image#cv2.resize(image, (340, 410))\n",
        "\n",
        "    # Read labels from the file\n",
        "    labels_df = pd.read_csv(labels_file)  # Ensure labels.txt is formatted correctly\n",
        "    image_name = image_path.split('/')[-1]  # Extract filename from path\n",
        "\n",
        "    # Find the row corresponding to the image name (assuming there is an 'id' or filename column)\n",
        "    if 'img_name' in labels_df.columns:\n",
        "        row = labels_df[labels_df['img_name'] == image_name]\n",
        "    else:\n",
        "        row = labels_df.iloc[0]  # If there's no ID column, just use the first row (for testing)\n",
        "\n",
        "    if row.empty:\n",
        "        print(f\"No labels found for {image_name}\")\n",
        "        return\n",
        "\n",
        "    # Extract ground truth coordinates\n",
        "    x1, y1, x2, y2 = row[['x1', 'y1', 'x2', 'y2']].to_numpy().flatten()\n",
        "    x1 = int(x1)\n",
        "    x2 = int(x2)\n",
        "    y1 = int(y1)\n",
        "    y2 = int(y2)\n",
        "    print(f\"Ground truth coordinates: x1={x1}, y1={y1}, x2={x2}, y2={y2}\")\n",
        "\n",
        "    # Draw red circles at the ground truth coordinates\n",
        "    image_drawn = image_resized.copy()\n",
        "    cv2.circle(image_drawn, (x1, y1), radius=5, color=(0, 255, 0), thickness=-1)  # Green circle\n",
        "    cv2.circle(image_drawn, (x2, y2), radius=5, color=(0, 0, 255), thickness=-1)  # Red circle\n",
        "\n",
        "    # Display the image with the drawn points\n",
        "    cv2.imshow(\"img\",image_drawn)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "image_path = f\"filtered_truncated_captchas/captcha_8.png\"\n",
        "labels_file = \"truncated_labels.csv\"\n",
        "draw_labels_on_image(image_path,labels_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing mean & std: 100%|██████████| 316/316 [00:00<00:00, 2012.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "moyenne = [214.72969 217.93405 141.7747 ] \n",
            " std=[51.95303  47.542946 46.740856]\n",
            "[214.72969 217.93405 141.7747 ] [51.95303  47.542946 46.740856]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Normalizing images: 100%|██████████| 316/316 [00:00<00:00, 367.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Normalized images saved in: normalized_images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm  # Progress bar\n",
        "\n",
        "def compute_mean_std(input_folder):\n",
        "    \"\"\"Compute the mean and std for each RGB channel across all images.\"\"\"\n",
        "    image_list = []\n",
        "    \n",
        "    for filename in tqdm(os.listdir(input_folder), desc=\"Computing mean & std\"):\n",
        "        img_path = os.path.join(input_folder, filename)\n",
        "        img = cv2.imread(img_path)  # Read image in BGR format\n",
        "        if img is not None:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "            image_list.append(img)\n",
        "\n",
        "    # Convert list to a big numpy array (N, H, W, C)\n",
        "    image_array = np.stack(image_list, axis=0).astype(np.float32) \n",
        "\n",
        "    # Compute mean and std along (N, H, W) axis → (C,)\n",
        "    mean = np.mean(image_array, axis=(0, 1, 2))\n",
        "    std = np.std(image_array, axis=(0, 1, 2))\n",
        "    print(f\"moyenne = {mean} \\n std={std}\")\n",
        "    return mean, std\n",
        "\n",
        "def normalize_images(input_folder, output_folder):\n",
        "    \"\"\"Normalize RGB channels of all images in input_folder and save them to output_folder.\"\"\"\n",
        "    os.makedirs(output_folder, exist_ok=True)  # Create output folder if not exists\n",
        "\n",
        "    # Compute mean and std\n",
        "    mean, std = compute_mean_std(input_folder)\n",
        "    print(mean,std)\n",
        "    for filename in tqdm(os.listdir(input_folder), desc=\"Normalizing images\"):\n",
        "        img_path = os.path.join(input_folder, filename)\n",
        "        img = cv2.imread(img_path)  # Read image in BGR format\n",
        "        if img is None:\n",
        "            print(f\"Skipping {filename} (unable to read)\")\n",
        "            continue\n",
        "\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "        img = img.astype(np.float32) \n",
        "\n",
        "        # Normalize: (pixel - mean) / std\n",
        "        img = (img - mean) / std\n",
        "\n",
        "        # Convert back to 0-255 range for saving\n",
        "        img = ((img - img.min()) / (img.max() - img.min()) * 255).astype(np.uint8)\n",
        "\n",
        "        # Save processed image\n",
        "        output_path = os.path.join(output_folder, filename)\n",
        "        cv2.imwrite(output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))  # Convert back to BGR\n",
        "\n",
        "    print(f\"✅ Normalized images saved in: {output_folder}\")\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "input_folder = \"truncated_captchas\"\n",
        "output_folder = \"normalized_images\"\n",
        "normalize_images(input_folder, output_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing new images: 100%|██████████| 316/316 [00:00<00:00, 880.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Normalized images saved in: normalized_premier_perso\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing new images: 100%|██████████| 316/316 [00:00<00:00, 889.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Normalized images saved in: normalized_second_perso\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "def apply_existing_normalization(input_folder, output_folder, mean, std):\n",
        "    \"\"\"\n",
        "    Applies the same mean and std normalization (computed earlier) to new images.\n",
        "\n",
        "    Args:\n",
        "        input_folder (str): Path to the folder containing new images.\n",
        "        output_folder (str): Path to save the normalized images.\n",
        "        mean (tuple): Mean values of the original dataset (R, G, B).\n",
        "        std (tuple): Standard deviation values of the original dataset (R, G, B).\n",
        "    \"\"\"\n",
        "    os.makedirs(output_folder, exist_ok=True)  # Create the output folder if it doesn't exist\n",
        "    df = pd.DataFrame([])\n",
        "    for filename in tqdm(os.listdir(input_folder), desc=\"Processing new images\"):\n",
        "        img_path = os.path.join(input_folder, filename)\n",
        "        img = cv2.imread(img_path)  # Read image in BGR format\n",
        "        if img is None:\n",
        "            print(f\"Skipping {filename} (unable to read)\")\n",
        "            continue\n",
        "        \n",
        "        # Convert to RGB and scale pixel values to [0,1]\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "\n",
        "        # Apply normalization using precomputed mean & std\n",
        "        img_normalized = (img - mean) / std  # Standardize the image\n",
        "\n",
        "        # Rescale to 0-255 for saving\n",
        "        img_normalized = ((img_normalized - img_normalized.min()) / (img_normalized.max() - img_normalized.min()) * 255).astype(np.uint8)\n",
        "        df = pd.concat([df, pd.DataFrame({\"img_name\": [filename], \"min\": [img_normalized.min()], \"max\": [img_normalized.max()]})], ignore_index=True)\n",
        "\n",
        "        # Convert back to BGR before saving (OpenCV expects BGR format)\n",
        "        output_path = os.path.join(output_folder, filename)\n",
        "        cv2.imwrite(output_path, cv2.cvtColor(img_normalized, cv2.COLOR_RGB2BGR))\n",
        "    df.to_csv(input_folder+\".csv\")\n",
        "    print(f\"✅ Normalized images saved in: {output_folder}\")\n",
        "\n",
        "# Valeurs trouvées:\n",
        "mean = (0.83977205, 0.8524061, 0.55467314)  # Dataset's mean\n",
        "std = (0.2027646, 0.18541439, 0.18301369)  # Dataset's std\n",
        "\n",
        "input_folder = \"premier_perso\"\n",
        "output_folder = \"normalized_premier_perso\"\n",
        "apply_existing_normalization(input_folder, output_folder, mean, std)\n",
        "\n",
        "input_folder = \"second_perso\"\n",
        "output_folder = \"normalized_second_perso\"\n",
        "apply_existing_normalization(input_folder, output_folder, mean, std)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def encode_image(img, original_min, original_max):\n",
        "    \"\"\"\n",
        "    Reverts the min-max scaling applied to an image.\n",
        "\n",
        "    Args:\n",
        "        img (numpy.ndarray): The encoded image (after min-max scaling to 255).\n",
        "        original_min (float): The minimum pixel value before encoding.\n",
        "        original_max (float): The maximum pixel value before encoding.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The encoded image with values standardized\n",
        "    \"\"\"\n",
        "    img = img.astype(np.float32)  # Convert to float32 for precision\n",
        "\n",
        "    # Reverse min-max scaling: \n",
        "    img_encoded = img / 255.0 * (original_max - original_min) + original_min\n",
        "\n",
        "    return img_encoded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_name</th>\n",
              "      <th>x1</th>\n",
              "      <th>y1</th>\n",
              "      <th>x2</th>\n",
              "      <th>y2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>captcha_1.png</td>\n",
              "      <td>-1.033635</td>\n",
              "      <td>0.793751</td>\n",
              "      <td>-0.911822</td>\n",
              "      <td>-1.717953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>captcha_10.png</td>\n",
              "      <td>-0.877801</td>\n",
              "      <td>0.750201</td>\n",
              "      <td>0.950419</td>\n",
              "      <td>-0.778652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>captcha_100.png</td>\n",
              "      <td>0.101729</td>\n",
              "      <td>1.708298</td>\n",
              "      <td>-1.505283</td>\n",
              "      <td>1.612295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>captcha_101.png</td>\n",
              "      <td>0.880900</td>\n",
              "      <td>0.380028</td>\n",
              "      <td>-0.809501</td>\n",
              "      <td>1.099949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>captcha_102.png</td>\n",
              "      <td>-0.054105</td>\n",
              "      <td>0.488902</td>\n",
              "      <td>0.950419</td>\n",
              "      <td>1.206688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>captcha_95.png</td>\n",
              "      <td>-0.298988</td>\n",
              "      <td>-1.383742</td>\n",
              "      <td>0.336493</td>\n",
              "      <td>1.441513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>captcha_96.png</td>\n",
              "      <td>-0.989111</td>\n",
              "      <td>-1.274867</td>\n",
              "      <td>-1.095999</td>\n",
              "      <td>1.249383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>captcha_97.png</td>\n",
              "      <td>-1.055897</td>\n",
              "      <td>1.272800</td>\n",
              "      <td>0.643456</td>\n",
              "      <td>-0.287654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>captcha_98.png</td>\n",
              "      <td>-1.055897</td>\n",
              "      <td>-0.011921</td>\n",
              "      <td>1.339238</td>\n",
              "      <td>-1.120216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>captcha_99.png</td>\n",
              "      <td>-1.122683</td>\n",
              "      <td>1.599424</td>\n",
              "      <td>-1.484819</td>\n",
              "      <td>0.181996</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>316 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            img_name        x1        y1        x2        y2\n",
              "0      captcha_1.png -1.033635  0.793751 -0.911822 -1.717953\n",
              "1     captcha_10.png -0.877801  0.750201  0.950419 -0.778652\n",
              "2    captcha_100.png  0.101729  1.708298 -1.505283  1.612295\n",
              "3    captcha_101.png  0.880900  0.380028 -0.809501  1.099949\n",
              "4    captcha_102.png -0.054105  0.488902  0.950419  1.206688\n",
              "..               ...       ...       ...       ...       ...\n",
              "311   captcha_95.png -0.298988 -1.383742  0.336493  1.441513\n",
              "312   captcha_96.png -0.989111 -1.274867 -1.095999  1.249383\n",
              "313   captcha_97.png -1.055897  1.272800  0.643456 -0.287654\n",
              "314   captcha_98.png -1.055897 -0.011921  1.339238 -1.120216\n",
              "315   captcha_99.png -1.122683  1.599424 -1.484819  0.181996\n",
              "\n",
              "[316 rows x 5 columns]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def standardize_columns(df, columns, output_file=\"mean_std.txt\"):\n",
        "    \"\"\"\n",
        "    Standardizes the selected columns of a DataFrame and saves the mean and std for each column in a text file.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame.\n",
        "        columns (list): List of column names to standardize.\n",
        "        output_file (str): File name to save the mean and std values.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with standardized columns.\n",
        "    \"\"\"\n",
        "    mean_std_values = {}\n",
        "\n",
        "    for col in columns:\n",
        "        mean = df[col].mean()\n",
        "        std = df[col].std()\n",
        "\n",
        "        if std == 0:  # Avoid division by zero\n",
        "            std = 1  \n",
        "\n",
        "        df[col] = (df[col] - mean) / std\n",
        "        mean_std_values[col] = {\"mean\": mean, \"std\": std}\n",
        "\n",
        "    # Save mean and std to a text file\n",
        "    with open(output_file, \"w\") as f:\n",
        "        for col, values in mean_std_values.items():\n",
        "            f.write(f\"{col} mean: {values['mean']}, std: {values['std']}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "df = pd.read_csv(\"truncated_labels.csv\")\n",
        "df = standardize_columns(df,df.columns[1:])\n",
        "df.to_csv(\"std_truncated_labels.csv\", index=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(210, 210, 3)\n",
            "(50, 50, 3)\n",
            "(50, 50, 3)\n"
          ]
        }
      ],
      "source": [
        "extract_dirs = ['normalized_images',\"normalized_premier_perso\",\"normalized_second_perso\"]\n",
        "for extract_dir in extract_dirs:\n",
        "    image_path = f\"{extract_dir}/captcha_8.png\"\n",
        "    image = cv2.imread(image_path)\n",
        "    print(image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Large image shape: torch.Size([8, 3, 210, 210])\n",
            "Small image 1 shape: torch.Size([8, 3, 50, 50])\n",
            "Small image 2 shape: torch.Size([8, 3, 50, 50])\n",
            "Labels: torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class CaptchaDataset(Dataset):\n",
        "    def __init__(self, extract_dirs, label_file, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            extract_dirs (list of str): List of directories where images are stored.\n",
        "            label_file (str): Path to the CSV file containing labels.\n",
        "            transform (callable, optional): Transformations to apply on images.\n",
        "        \"\"\"\n",
        "        self.extract_dirs = extract_dirs  # List of directories\n",
        "        self.labels = pd.read_csv(label_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def normalize_image(self,image, mean=(0.83977205, 0.8524061, 0.55467314), std=(0.2027646, 0.18541439,0.18301369)):\n",
        "        \"\"\"\n",
        "        Normalize the image with given mean and std for each channel.\n",
        "        \n",
        "        Args:\n",
        "            image (np.array): The input image to be normalized.\n",
        "            mean (tuple): A tuple containing the mean for each channel (R, G, B).\n",
        "            std (tuple): A tuple containing the standard deviation for each channel (R, G, B).\n",
        "            \n",
        "        Returns:\n",
        "            np.array: The normalized image.\n",
        "        \"\"\"\n",
        "        # Normalize image by subtracting mean and dividing by std for each channel (RGB)\n",
        "        image = (image - mean) / std\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image ID\n",
        "        img_id = self.labels.iloc[idx]['img_name']\n",
        "        \n",
        "        # Initialize paths for the three parts\n",
        "        large_img_path = None\n",
        "        small_img1_path = None\n",
        "        small_img2_path = None\n",
        "\n",
        "        # Construct the paths for the images\n",
        "        large_img_path = os.path.join(self.extract_dirs[0], img_id)\n",
        "        small_img1_path = os.path.join(self.extract_dirs[1], img_id)\n",
        "        small_img2_path = os.path.join(self.extract_dirs[2], img_id)\n",
        "\n",
        "        # Check if we found all parts\n",
        "        if not os.path.exists(large_img_path) or not os.path.exists(small_img1_path) or not os.path.exists(small_img2_path):\n",
        "            print(f\"Missing images for {img_id}!\")\n",
        "            return None\n",
        "\n",
        "        # Load images (big and small)\n",
        "        large_img = cv2.imread(large_img_path)\n",
        "        small_img1 = cv2.imread(small_img1_path)\n",
        "        small_img2 = cv2.imread(small_img2_path)\n",
        "\n",
        "        # Convert to RGB (OpenCV loads as BGR by default)\n",
        "        large_img = cv2.cvtColor(large_img, cv2.COLOR_BGR2RGB)\n",
        "        small_img1 = cv2.cvtColor(small_img1, cv2.COLOR_BGR2RGB)\n",
        "        small_img2 = cv2.cvtColor(small_img2, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Apply normalization using the provided mean and std\n",
        "        large_img = self.normalize_image(large_img)\n",
        "        small_img1 = self.normalize_image(small_img1)\n",
        "        small_img2 = self.normalize_image(small_img2)\n",
        "\n",
        "        # Transpose to PyTorch format (C, H, W)\n",
        "        large_img = np.transpose(large_img, (2, 0, 1))\n",
        "        small_img1 = np.transpose(small_img1, (2, 0, 1))\n",
        "        small_img2 = np.transpose(small_img2, (2, 0, 1))\n",
        "\n",
        "        # Get labels (x1, y1, x2, y2)\n",
        "        labels = self.labels.iloc[idx][['x1', 'y1', 'x2', 'y2']].values.astype(np.float32)\n",
        "\n",
        "        # Convert to tensors\n",
        "        large_img = torch.tensor(large_img)\n",
        "        small_img1 = torch.tensor(small_img1)\n",
        "        small_img2 = torch.tensor(small_img2)\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "        return large_img, small_img1, small_img2, labels\n",
        "\n",
        "# Example Usage\n",
        "extract_dirs = [\"normalized_images\", \"normalized_premier_perso\", \"normalized_second_perso\"]\n",
        "label_file = \"std_truncated_labels.csv\"\n",
        "\n",
        "dataset = CaptchaDataset(extract_dirs, label_file)\n",
        "train_loader = DataLoader(dataset, batch_size=8,shuffle=True)\n",
        "\n",
        "# Test data loading\n",
        "for large_img, small_img1, small_img2, labels in train_loader:\n",
        "    if large_img is not None:\n",
        "        print(\"Large image shape:\", large_img.shape)   # Expected: (8, 3, 210, 210)\n",
        "        print(\"Small image 1 shape:\", small_img1.shape)  # Expected: (8, 3, 50, 50)\n",
        "        print(\"Small image 2 shape:\", small_img2.shape)  # Expected: (8, 3, 50, 50)\n",
        "        print(\"Labels:\", labels.shape)  # Expected: (8, 4)\n",
        "    break  # Just checking one batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_name</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>captcha_1.png</td>\n",
              "      <td>0.222933</td>\n",
              "      <td>0.696238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>captcha_10.png</td>\n",
              "      <td>0.259910</td>\n",
              "      <td>0.685673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>captcha_100.png</td>\n",
              "      <td>0.492337</td>\n",
              "      <td>0.918099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>captcha_101.png</td>\n",
              "      <td>0.677221</td>\n",
              "      <td>0.595872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>captcha_102.png</td>\n",
              "      <td>0.455360</td>\n",
              "      <td>0.622284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>captcha_95.png</td>\n",
              "      <td>0.397253</td>\n",
              "      <td>0.167996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>captcha_96.png</td>\n",
              "      <td>0.233498</td>\n",
              "      <td>0.194408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>captcha_97.png</td>\n",
              "      <td>0.217651</td>\n",
              "      <td>0.812451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>captcha_98.png</td>\n",
              "      <td>0.217651</td>\n",
              "      <td>0.500788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>captcha_99.png</td>\n",
              "      <td>0.201804</td>\n",
              "      <td>0.891687</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>316 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            img_name         x         y\n",
              "0      captcha_1.png  0.222933  0.696238\n",
              "1     captcha_10.png  0.259910  0.685673\n",
              "2    captcha_100.png  0.492337  0.918099\n",
              "3    captcha_101.png  0.677221  0.595872\n",
              "4    captcha_102.png  0.455360  0.622284\n",
              "..               ...       ...       ...\n",
              "311   captcha_95.png  0.397253  0.167996\n",
              "312   captcha_96.png  0.233498  0.194408\n",
              "313   captcha_97.png  0.217651  0.812451\n",
              "314   captcha_98.png  0.217651  0.500788\n",
              "315   captcha_99.png  0.201804  0.891687\n",
              "\n",
              "[316 rows x 3 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd \n",
        "df = pd.read_csv(\"truncated_labels.csv\")\n",
        "df=df.drop([\"x2\",\"y2\"],axis=1)\n",
        "df.columns = [\"img_name\",\"x\",\"y\"]\n",
        "# Normalize x and y coordinates (assuming max width = 210, max height = 210)\n",
        "df[\"x\"] = df[\"x\"] / 210.0\n",
        "df[\"y\"] = df[\"y\"] / 210.0\n",
        "\n",
        "df.to_csv(\"labels.csv\",index=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Field image batch shape: torch.Size([8, 3, 210, 210])\n",
            "Draw image batch shape: torch.Size([8, 3, 50, 50])\n",
            "Label batch shape: torch.Size([8, 2])\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "class CaptchaDetectionDataset(Dataset):\n",
        "    def __init__(self, field_folder, draw_folder, label_csv, \n",
        "                 transform_field=None, transform_draw=None, transform_label=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            field_folder (str): Path to the folder containing field images (210x210).\n",
        "            draw_folder (str): Path to the folder containing draw images (50x50).\n",
        "            label_csv (str): Path to the CSV file containing labels with columns:\n",
        "                             \"img_name\", \"x\", \"y\".\n",
        "            transform_field (callable, optional): Transform to apply to field images.\n",
        "            transform_draw (callable, optional): Transform to apply to draw images.\n",
        "            transform_label (callable, optional): Transform to apply to labels.\n",
        "        \"\"\"\n",
        "        self.field_folder = field_folder\n",
        "        self.draw_folder = draw_folder\n",
        "        self.labels_df = pd.read_csv(label_csv)\n",
        "        \n",
        "        # Optional transforms for images/labels\n",
        "        self.transform_field = transform_field\n",
        "        self.transform_draw = transform_draw\n",
        "        self.transform_label = transform_label\n",
        "        \n",
        "        # Create a list of image names from the CSV\n",
        "        self.img_names = self.labels_df[\"img_name\"].tolist()\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Get the image file name from CSV\n",
        "        img_name = self.img_names[idx]\n",
        "        \n",
        "        # Construct full paths for field and draw images\n",
        "        field_img_path = os.path.join(self.field_folder, img_name)\n",
        "        draw_img_path = os.path.join(self.draw_folder, img_name)\n",
        "        \n",
        "        # Load images using PIL and convert them to RGB\n",
        "        field_img = Image.open(field_img_path).convert('RGB')\n",
        "        draw_img = Image.open(draw_img_path).convert('RGB')\n",
        "        \n",
        "        # Apply transformations if provided (for example, resizing, normalization, converting to tensor)\n",
        "        if self.transform_field:\n",
        "            field_img = self.transform_field(field_img)\n",
        "        if self.transform_draw:\n",
        "            draw_img = self.transform_draw(draw_img)\n",
        "        \n",
        "        # Extract label (x, y) from the CSV row and convert to a float tensor\n",
        "        row = self.labels_df.iloc[idx]\n",
        "        x = float(row[\"x\"])\n",
        "        y = float(row[\"y\"])\n",
        "        label = torch.tensor([x, y], dtype=torch.float32)\n",
        "        if self.transform_label:\n",
        "            label = self.transform_label(label)\n",
        "        \n",
        "        return field_img, draw_img, label\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "    # Define transforms: Convert images to tensors, resize if needed, etc.\n",
        "    transform_field = transforms.Compose([\n",
        "        transforms.Resize((210, 210)),\n",
        "        transforms.ToTensor()  # Converts to tensor and scales pixel values to [0, 1]\n",
        "    ])\n",
        "    \n",
        "    transform_draw = transforms.Compose([\n",
        "        transforms.Resize((50, 50)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    \n",
        "    # Folder paths\n",
        "    field_folder = \"filtered_truncated_captchas\"   # e.g., images of size 210x210\n",
        "    draw_folder = \"normalized_premier_perso\"       # e.g., images of size 50x50\n",
        "    label_csv = \"labels.csv\"          # CSV with columns: \"img_name\", \"x\", \"y\"\n",
        "    \n",
        "    # Create dataset\n",
        "    dataset = CaptchaDetectionDataset(field_folder, draw_folder, label_csv,\n",
        "                                      transform_field=transform_field,\n",
        "                                      transform_draw=transform_draw)\n",
        "    \n",
        "    # Create DataLoader for batching and shuffling\n",
        "    from torch.utils.data import DataLoader\n",
        "    data_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "    \n",
        "    # Iterate over one batch and print shapes\n",
        "    for field_img, draw_img, label in data_loader:\n",
        "        print(\"Field image batch shape:\", field_img.shape)  # Expected: (B, 3, 210, 210)\n",
        "        print(\"Draw image batch shape:\", draw_img.shape)    # Expected: (B, 3, 50, 50)\n",
        "        print(\"Label batch shape:\", label.shape)            # Expected: (B, 2)\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 252\n",
            "Test dataset size: 64\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "# Define train-test split ratio\n",
        "train_ratio = 0.8  # 80% train, 20% test\n",
        "test_ratio = 1 - train_ratio\n",
        "\n",
        "# Compute sizes\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(train_ratio * dataset_size)\n",
        "test_size = dataset_size - train_size  # Ensures all samples are used\n",
        "\n",
        "# Split dataset\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "# Define train-test split ratio\n",
        "train_ratio = 0.8  # 80% train, 20% test\n",
        "test_ratio = 1 - train_ratio\n",
        "\n",
        "# Compute sizes\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(train_ratio * dataset_size)\n",
        "test_size = dataset_size - train_size  # Ensures all samples are used\n",
        "\n",
        "# Split dataset\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the model (using the DrawLocatorNet defined previously)\n",
        "model = DrawLocatorNet().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()  # Since we're doing coordinate regression\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for field_img, draw_img, labels in train_loader:\n",
        "        # Move data to device and ensure dtype is float32\n",
        "        field_img = field_img.to(device).float()\n",
        "        draw_img = draw_img.to(device).float()\n",
        "        labels = labels.to(device).float()  # Expected shape: (B,2)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(field_img, draw_img)  # Expected shape: (B, 2)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * field_img.size(0)\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_train_loss:.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for field_img, draw_img, labels in val_loader:\n",
        "            field_img = field_img.to(device).float()\n",
        "            draw_img = draw_img.to(device).float()\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            outputs = model(field_img, draw_img)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item() * field_img.size(0)\n",
        "\n",
        "    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "# Optionally, save the trained model\n",
        "torch.save(model.state_dict(), \"draw_locator_net.pth\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
